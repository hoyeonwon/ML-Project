{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DLiP Group 1: Malaria Detection Using Machine Learning","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n\n* [Introduction](#section-one)\n* [Data Preprocessing](#section-two)\n* [Model Fitting](#section-three)\n    - [Basic Convolutional Network from scratch](#subsection-one)\n    - [Pretrained Convolutional Network 1](#subsection-two)\n        - [Model Improvement](#subsection-two-one)\n    - [Pretrained Convolutional Network 2](#subsection-three)\n        - [Model Improvement](#subsection-three-one)\n    - [Customized Convolutional Network](#subsection-four)\n* [Results](#section-four)\n    - [Model Interpretation](#subsection-fourone)\n* [Conclusion & Discussion](#section-five)\n* [References](#section-six)\n* [Member Contribution](#section-seven)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# 1. Introduction\n\n## Data considerations\n\nThis project aims to build an algorithm to detect malaria parasites from thin blood smear images. This project was adopted from the study of Sivaramakrishnan et al. (2018) and the article from Sarkar (2019), by using the image dataset from the official NIH website. Our motivation is to replicate or improve the performance of the findings of the aforementioned literature. Therefore, the research question of this project is:\n\n> <h4> Can we detect malaria parasite from blood images? </h4>   \n\nFirst, we would like to give an overview of the data:\n\n**1. Where does the data come from?**\n* The data comes from the official [National Library of Medicine (NIH) website](https://ceb.nlm.nih.gov/repositories/malaria-datasets/), which provides a dataset for supporting automated image processing on clinical decision-making in disease screening and diagnostics.\n* Since we only use thin smear images, rather than thick ones; our automated processing might not be generalizable to all kinds of malaria parasites. Additionally, we are not using other similar parasite-infected blood images to differentiate the training process, meaning it could be possible for the network to confuse with other diseases in other sets of blood images.\n\n**2. What are candidate machine learning methods?**\n\n* We have based our decision on Sivaramakrishnan et al. (2018) and Sarkar (2019):\n<div style=\"display:flex; flex-direction: row; flex-wrap: nowrap; align-items: stretch; width:100%;\">\n    <div>\n        <ul>\n<h5> Model Candidates: </h5>          \n<li> Basic convolutional neural network from scratch\n<li> Pre-trained convolutional neural network\n<li> Pre-trained convolutional neural network with Image Augmentation\n<li> Customed convolutional neural network\n    </div>","metadata":{}},{"cell_type":"markdown","source":"**3. What is the Bayes' error bound? (Any guestimate from scientific literature or web resources?)** \n* To have an idea of a lower bound on the Bayes bound, the best 'machine' we have to identify malaria parasites from blood images is human. The most popular method for Malaria diagnostic tests is the polymerase chain reaction (PCR). According to the findings of Feleke, Alemu, and Yemanebirhane (2021), the average performance on several tests was:\n\n\n| AUC  |  SENSITIVITY  |  SPECIFICITY |\n|---------:|--------:|--------:|\n|  83.00%   | 75.20%   | 97.12%   |\n\n--------------------------------","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# 2. Data Preprocessing\n\nThe current step will involve:\n1. Importing necessary packages and loading the data into a data frame;\n2. Checking image size and quality;\n3. Resizing images;\n4. Splitting into training, validation, and a test data","metadata":{}},{"cell_type":"markdown","source":"## Import packages & Load data\nAll the packages necessary for the project are loaded below:","metadata":{}},{"cell_type":"code","source":"#import packages\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nimport tensorflow as tf\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport glob\nimport PIL\nimport os.path\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nimport pandas as pd\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental import preprocessing\nimport sklearn.metrics as skmet\nimport itertools\nfrom keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:04:34.738531Z","iopub.execute_input":"2022-12-20T17:04:34.739091Z","iopub.status.idle":"2022-12-20T17:04:41.424598Z","shell.execute_reply.started":"2022-12-20T17:04:34.739021Z","shell.execute_reply":"2022-12-20T17:04:41.423547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below, we have specified paths for 1) infected and 2) uninfected files which contain blood images with malaria parasites and images without the parasite respectively. Then, the data frame is defined with both files included, which is shown below.","metadata":{}},{"cell_type":"code","source":"#Load in the data \ninfected_path = \"/kaggle/input/cell-images-for-detecting-malaria/cell_images/Parasitized\"\nuninfected_path = \"/kaggle/input/cell-images-for-detecting-malaria/cell_images/Uninfected\"\ninfected_files = glob.glob(infected_path+'/*.png')\nuninfected_files = glob.glob(uninfected_path+'/*.png')\n\nnp.random.seed(42)\n\nfiles_df = pd.DataFrame({\n    'filename': infected_files + uninfected_files,\n    'label': ['malaria'] * len(infected_files) + ['healthy'] * len(uninfected_files)\n}).sample(frac=1, random_state=42).reset_index(drop=True)\n\nfiles_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:04:44.735226Z","iopub.execute_input":"2022-12-20T17:04:44.736045Z","iopub.status.idle":"2022-12-20T17:04:45.821176Z","shell.execute_reply.started":"2022-12-20T17:04:44.736008Z","shell.execute_reply":"2022-12-20T17:04:45.820265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check image size and quality\n\nNow, we will check whether the image was loaded correctly. The following code will provide several images that indicate the color and quality of the images.","metadata":{}},{"cell_type":"code","source":"# code for displaying multiple images in one figure\n# create figure\nfig = plt.figure(figsize = (10, 7))\n\n# setting values to rows and column variables\nrows = 2\ncolumns = 3\n\n# reading images\nM1 = Image.open(infected_files[567])\nM2 = Image.open(infected_files[4783])\nM3 = Image.open(infected_files[89])\nH1 = Image.open(uninfected_files[20])\nH2 = Image.open(uninfected_files[678])\nH3 = Image.open(uninfected_files[472])\n\n\n# Adds a subplot at the 1st position\nfig.add_subplot(rows, columns, 1)\n\n# showing image\nplt.imshow(M1)\nplt.axis(\"off\")\nplt.title(\"Malaria\")\n\n# Adds a subplot at the 2nd position\nfig.add_subplot(rows, columns, 2)\n\n# showing image\nplt.imshow(M2)\nplt.axis(\"off\")\nplt.title(\"Malaria\")\n\n# Adds a subplot at the 3rd position\nfig.add_subplot(rows, columns, 3)\n\n# showing image\nplt.imshow(M3)\nplt.axis(\"off\")\nplt.title(\"Malaria\")\n\n# Adds a subplot at the 4th position\nfig.add_subplot(rows, columns, 4)\n\n# showing image\nplt.imshow(H1)\nplt.axis(\"off\")\nplt.title(\"Healthy\")\n\n# Adds a subplot at the 5th position\nfig.add_subplot(rows, columns, 5)\n\n# showing image\nplt.imshow(H2)\nplt.axis(\"off\")\nplt.title(\"Healthy\")\n\n# Adds a subplot at the 6th position\nfig.add_subplot(rows, columns, 6)\n\n# showing image\nplt.imshow(H3)\nplt.axis(\"off\")\nplt.title(\"Healthy\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:04:48.222855Z","iopub.execute_input":"2022-12-20T17:04:48.225545Z","iopub.status.idle":"2022-12-20T17:04:48.845468Z","shell.execute_reply.started":"2022-12-20T17:04:48.225501Z","shell.execute_reply":"2022-12-20T17:04:48.844229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the quality seems appropriate, we would like to check if the size of the images are the same and appropriate to proceed with model fitting.","metadata":{}},{"cell_type":"code","source":"# Check if the images are the same size\n\n# get width and height\nwidth1 = M1.width\nheight1 = M1.height\n  \n# display width and height\nprint(\"The height of image 1 is: \", height1)\nprint(\"The width of image 1 is: \", width1)\n\n# get width and height\nwidth2 = M2.width\nheight2 = M2.height\n  \n# display width and height\nprint(\"The height of image 2 is: \", height2)\nprint(\"The width of image 2 is: \", width2)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:04:52.819902Z","iopub.execute_input":"2022-12-20T17:04:52.820289Z","iopub.status.idle":"2022-12-20T17:04:52.827211Z","shell.execute_reply.started":"2022-12-20T17:04:52.820253Z","shell.execute_reply":"2022-12-20T17:04:52.826293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen from the outputs, the images are not the same size so first we need to resize them. As we have quite a lot of data, we would like to resize them to **50 x 50** pixels which would reduce the working memory of the computer and allow faster model fitting. Also, in order to correctly measure for the performance of the model, we scaled the data by dividing the data by **255** (as in the article by Sarkar (2019)).","metadata":{}},{"cell_type":"code","source":"resized_df = []\nlabels_names = []\nfor file in range(0, len(files_df)):\n    img_array = cv2.imread(files_df.iloc[file,0])\n    img = Image.fromarray(img_array, 'RGB')\n    img_resized = img.resize((50,50), Image.ANTIALIAS)\n    resized_df.append(np.array(img_resized))\n    labels_names.append(files_df.iloc[file,1])\n\n# recode labels to numbers instead of names\nlabels_recoded = []\nfor i in labels_names:\n    if i == \"malaria\":\n        labels_recoded.append(1)\n    else:\n        labels_recoded.append(0)\n\n\n# change the data and the labels to arrays so it's suitable for the CNN\ndata = np.array(resized_df)\nlabels = np.array(labels_recoded)\n\n# change to datatypes suitable for the CNN\ndata = data.astype(np.float32) \nlabels = labels.astype(np.int32) \n\n# scale the data\ndata = data/255","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:04:55.892034Z","iopub.execute_input":"2022-12-20T17:04:55.892568Z","iopub.status.idle":"2022-12-20T17:11:47.245509Z","shell.execute_reply.started":"2022-12-20T17:04:55.89252Z","shell.execute_reply":"2022-12-20T17:11:47.244495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can check if the resizing went well:","metadata":{}},{"cell_type":"code","source":"#Check if resizing worked\nprint(\"There are\", data.shape[0], \"pictures.\") #All picteres are there!\nprint(\"All picture have a width of\", data.shape[1], \"and a length of\", data.shape[2], \"pixels.\")\nprint(\"And all picture have\", data.shape[3], \"color channels.\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:11:47.24739Z","iopub.execute_input":"2022-12-20T17:11:47.247852Z","iopub.status.idle":"2022-12-20T17:11:47.255937Z","shell.execute_reply.started":"2022-12-20T17:11:47.247812Z","shell.execute_reply":"2022-12-20T17:11:47.254902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All pictures are there and the resizing worked. As an example, the following picture shows how the pictures look after resizing","metadata":{}},{"cell_type":"code","source":"# get image\nimg = data[1]\n\n# show the image\nplt.imshow(img)\nplt.axis('off')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:11:47.257279Z","iopub.execute_input":"2022-12-20T17:11:47.258389Z","iopub.status.idle":"2022-12-20T17:11:47.345427Z","shell.execute_reply.started":"2022-12-20T17:11:47.258353Z","shell.execute_reply":"2022-12-20T17:11:47.344303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting data\n\nTo avoid overfitting (the model fits the training data too well), we split the data into training, validation, and test data using 60:10:30 ratios. This will ensure that our model is performing well and not losing any rich information.","metadata":{}},{"cell_type":"code","source":"# split data set into train, validation and test datset using 60:10:30 split\ntrain_data, test_data, train_labels, test_labels = train_test_split(data,\n                                                                    labels, \n                                                                    test_size = 0.3,\n                                                                    random_state = 42)\ntrain_data, val_data, train_labels, val_labels = train_test_split(train_data,\n                                                                  train_labels,\n                                                                  test_size = 0.1,\n                                                                  random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:12:23.547733Z","iopub.execute_input":"2022-12-20T17:12:23.548185Z","iopub.status.idle":"2022-12-20T17:12:24.011106Z","shell.execute_reply.started":"2022-12-20T17:12:23.548137Z","shell.execute_reply":"2022-12-20T17:12:24.010036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train files:\", len(train_data),\"\\nVal files:\", len(val_data),\"\\nTest files:\", len(test_data))\nprint('Train labels:', len(train_labels), '\\nVal labels:', len(val_labels), '\\nTest labels:', len(test_labels))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:12:25.978525Z","iopub.execute_input":"2022-12-20T17:12:25.978881Z","iopub.status.idle":"2022-12-20T17:12:25.98435Z","shell.execute_reply.started":"2022-12-20T17:12:25.978851Z","shell.execute_reply":"2022-12-20T17:12:25.983001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# 3. Model Fitting\n\nIn total, we fitted four models, where the first 3 models were adopted from Sarkar (2019) and the last model was customized to increase performance on model fitting. The order follows:\n1. Basic Convolutional Network from scratch\n2. Pretrained Convolutional Network (VGG-19)\n3. Pretrained Convolutional Network with Data Augmentation and Fine Tuning\n4. Customized Convolutional Network\n\nBefore diving into the specifics of each model, we would like to mention that for all the models, the input_shape is the same as we don't change the images anymore. Therefore, we already specify the input shape for all the models.","metadata":{}},{"cell_type":"code","source":"input_shape = [50, 50, 3]","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:12:29.22152Z","iopub.execute_input":"2022-12-20T17:12:29.222628Z","iopub.status.idle":"2022-12-20T17:12:29.22752Z","shell.execute_reply.started":"2022-12-20T17:12:29.222585Z","shell.execute_reply":"2022-12-20T17:12:29.226586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-one\"></a>\n## 3.1 Basic Convolutional Network from scratch\n\n#### Model Architecture\nThe model is specified according to Sarkar (2019); using three types of layers: Convolutional, Pooling, and Fully Connected layers.","metadata":{}},{"cell_type":"code","source":"model1 = keras.Sequential([\n    # Input Shape\n    layers.InputLayer(input_shape = input_shape),\n\n    # First Convolutional Block\n    layers.Conv2D(filters = 32, kernel_size = 3, activation = \"relu\", padding = \"same\"),\n    layers.MaxPool2D(),\n\n    # Second Convolutional Block\n    layers.Conv2D(filters = 64, kernel_size = 3, activation = \"relu\", padding = \"same\"),\n    layers.MaxPool2D(),\n\n    # Third Convolutional Block\n    layers.Conv2D(filters = 128, kernel_size = 3, activation = \"relu\", padding = \"same\"),\n    layers.MaxPool2D(),\n\n    # Classifier Head\n    layers.Flatten(),\n    layers.Dense(units = 512, activation = \"relu\"),\n    layers.Dropout(rate = 0.3),\n    layers.Dense(units = 512, activation = \"relu\"),\n    layers.Dropout(rate = 0.3),\n    layers.Dense(1, activation = \"sigmoid\")\n])\nmodel1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:12:36.129204Z","iopub.execute_input":"2022-12-20T17:12:36.129785Z","iopub.status.idle":"2022-12-20T17:12:36.20012Z","shell.execute_reply.started":"2022-12-20T17:12:36.129749Z","shell.execute_reply":"2022-12-20T17:12:36.198582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train the model\nNow, we use training data and validation data to fit model 1.\nDuring the training process, we had one issue which was having the warnings of \"bad runs\" and \"faulty starting values\". Because of these, we have saved a model, using `model1.save` code, which resulted from a good run to ensure that it still can be used for further evaluation of the model (in the result section).","metadata":{}},{"cell_type":"code","source":"model1.compile(\n    optimizer = \"adam\",\n    loss = \"binary_crossentropy\",\n    metrics = [\"accuracy\"]\n)\n\nhistory1 = model1.fit(\n    train_data,\n    train_labels,\n    validation_data = (val_data, val_labels),\n    batch_size = 64,\n    epochs = 25,\n    verbose = 1,\n)\n\nmodel1.save(\"Model1.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:12:51.832656Z","iopub.execute_input":"2022-12-20T17:12:51.833004Z","iopub.status.idle":"2022-12-20T17:13:38.713971Z","shell.execute_reply.started":"2022-12-20T17:12:51.832973Z","shell.execute_reply":"2022-12-20T17:13:38.712888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot\nThen, we plot the model performance by comparing the accuracy and loss of train and validation sets.\n","metadata":{}},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\nt = f.suptitle(\"Model 1 Performance\", fontsize = 12)\nf.subplots_adjust(top = 0.85, wspace = 0.3)\n\nmax_epoch = len(history1.history[\"accuracy\"])+1\nepoch_list = list(range(1, max_epoch))\nax1.plot(epoch_list, history1.history[\"accuracy\"], label = \"Train Accuracy\")\nax1.plot(epoch_list, history1.history[\"val_accuracy\"], label = \"Validation Accuracy\")\nax1.set_xticks(np.arange(1, max_epoch, 5))\nax1.set_yticks(np.arange(0.75, 1, 0.02))\nax1.set_ylabel(\"Accuracy Value\")\nax1.set_xlabel(\"Epoch\")\nax1.set_title(\"Accuracy\")\nl1 = ax1.legend(loc = 4)\n\nax2.plot(epoch_list, history1.history[\"loss\"], label = \"Train Loss\")\nax2.plot(epoch_list, history1.history[\"val_loss\"], label = \"Validation Loss\")\nax2.set_xticks(np.arange(1, max_epoch, 5))\nax2.set_yticks(np.arange(0.0, 0.5, 0.05))\nax2.set_ylabel(\"Loss Value\")\nax2.set_xlabel(\"Epoch\")\nax2.set_title(\"Loss\")\nl2 = ax2.legend(loc = \"best\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:13:40.681391Z","iopub.execute_input":"2022-12-20T17:13:40.681731Z","iopub.status.idle":"2022-12-20T17:13:41.038354Z","shell.execute_reply.started":"2022-12-20T17:13:40.681702Z","shell.execute_reply":"2022-12-20T17:13:41.037448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that the training and validation accuracy are both high (around 0.95), and after a certain amount of epochs (around 20), the training accuracy reaches an accuracy of (almost) 1.\nWhen we observe validation loss, we see that after 6 epochs, it exponentially increases which indicates that the model is overfitting.","metadata":{}},{"cell_type":"code","source":"history1_frame = pd.DataFrame(history1.history)\nprint(\"Model 1 maximum validation accuracy: {}\".format(history1_frame[\"val_accuracy\"].max()))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:13:57.009764Z","iopub.execute_input":"2022-12-20T17:13:57.010117Z","iopub.status.idle":"2022-12-20T17:13:57.020812Z","shell.execute_reply.started":"2022-12-20T17:13:57.010087Z","shell.execute_reply":"2022-12-20T17:13:57.019504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-two\"></a>\n## 3.2 Pretrained Convolutional Network (VGG-19)\n\n#### Model Architecture\nWe use the VGG-19 model that is defined in the article. This model architecture contains 19 layers (convolutional and fully connected layers). The original model has a total of 16 convolution layers with 3x3 filters, and a total of two fully connected layers of 4096 units in each layer followed by a dense layer of 1000 units. The article, however, removed the last three layers from the original VGG-19 model so that it is used more effectively as a feature extractor.\n","metadata":{}},{"cell_type":"code","source":"# loading the VGG-19 model \nvgg = tf.keras.applications.vgg19.VGG19(include_top = False, weights = \"imagenet\", \n                                        input_shape = input_shape)\n\n# Freeze the layers\nfor layer in vgg.layers:\n    layer.trainable = False\n    \n# The pretrained base should not be trainable\n# because otherwise the pre-trained weights would be updated with backpropagation\n# better: fine-tune the model later \nvgg.trainable = False\n    \n# Attach head of Dense layers to perform the classification\n# based on article, and added layers.BatchNormalization()!\nmodel2 = keras.Sequential([\n    vgg,\n    layers.Flatten(),\n    layers.Dense(units = 512, activation = \"relu\"),\n    layers.Dropout(rate = 0.3),\n    layers.Dense(units = 512, activation = \"relu\"),\n    layers.Dropout(rate = 0.3),\n    layers.Dense(1, activation = \"sigmoid\")\n])","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:14:07.034118Z","iopub.execute_input":"2022-12-20T17:14:07.0345Z","iopub.status.idle":"2022-12-20T17:14:11.230686Z","shell.execute_reply.started":"2022-12-20T17:14:07.03447Z","shell.execute_reply":"2022-12-20T17:14:11.229542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total Layers:\", len(model2.layers))\nprint(\"Total trainable layers:\", \n      sum([1 for l in model2.layers if l.trainable]))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:14:15.967987Z","iopub.execute_input":"2022-12-20T17:14:15.968575Z","iopub.status.idle":"2022-12-20T17:14:15.977551Z","shell.execute_reply.started":"2022-12-20T17:14:15.968531Z","shell.execute_reply":"2022-12-20T17:14:15.97608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train the model","metadata":{}},{"cell_type":"code","source":"# specify loss function to be minimized & performance metrics\nmodel2.compile(optimizer = tf.keras.optimizers.RMSprop(lr=1e-4),\n                loss = \"binary_crossentropy\",\n                metrics = [\"accuracy\"])\nmodel2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:14:19.315826Z","iopub.execute_input":"2022-12-20T17:14:19.316182Z","iopub.status.idle":"2022-12-20T17:14:19.334014Z","shell.execute_reply.started":"2022-12-20T17:14:19.316144Z","shell.execute_reply":"2022-12-20T17:14:19.332732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history2 = model2.fit(train_data,\n                    train_labels, \n                    validation_data = (val_data,val_labels),\n                    batch_size = 64,\n                    epochs = 25,\n                    verbose = 1)\n\nmodel2.save(\"Model2.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:14:24.254141Z","iopub.execute_input":"2022-12-20T17:14:24.254834Z","iopub.status.idle":"2022-12-20T17:16:21.53141Z","shell.execute_reply.started":"2022-12-20T17:14:24.254799Z","shell.execute_reply":"2022-12-20T17:16:21.530313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot\nNow the model is fitted, we will check the model performance by plotting the accuracy and loss of both training and validation sets.","metadata":{}},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\nt = f.suptitle(\"Model 2 Performance\", fontsize = 12)\nf.subplots_adjust(top = 0.85, wspace = 0.3)\n\nmax_epoch = len(history2.history[\"accuracy\"])+1\nepoch_list = list(range(1, max_epoch))\nax1.plot(epoch_list, history2.history[\"accuracy\"], label = \"Train Accuracy\")\nax1.plot(epoch_list, history2.history[\"val_accuracy\"], label = \"Validation Accuracy\")\nax1.set_xticks(np.arange(1, max_epoch, 5))\nax1.set_yticks(np.arange(0.75, 1, 0.02))\nax1.set_ylabel(\"Accuracy Value\")\nax1.set_xlabel(\"Epoch\")\nax1.set_title(\"Accuracy\")\nl1 = ax1.legend(loc = 4)\n\nax2.plot(epoch_list, history2.history[\"loss\"], label = \"Train Loss\")\nax2.plot(epoch_list, history2.history[\"val_loss\"], label = \"Validation Loss\")\nax2.set_xticks(np.arange(1, max_epoch, 5))\nax2.set_yticks(np.arange(0.0, 0.5, 0.05))\nax2.set_ylabel(\"Loss Value\")\nax2.set_xlabel(\"Epoch\")\nax2.set_title(\"Loss\")\nl2 = ax2.legend(loc = \"best\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:16:22.065668Z","iopub.execute_input":"2022-12-20T17:16:22.067977Z","iopub.status.idle":"2022-12-20T17:16:22.531858Z","shell.execute_reply.started":"2022-12-20T17:16:22.067939Z","shell.execute_reply":"2022-12-20T17:16:22.531065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot, we see that the validation accuracy is quite wiggly and unstable. The accuracy seemed to have dropped compared to that of the first basic CNN model, although the trend of overfitting seems to be less than before.","metadata":{}},{"cell_type":"code","source":"history2_frame = pd.DataFrame(history2.history)\nprint(\"Model 2 maximum validation accuracy: {}\".format(history2_frame[\"val_accuracy\"].max()))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:16:32.363997Z","iopub.execute_input":"2022-12-20T17:16:32.364385Z","iopub.status.idle":"2022-12-20T17:16:32.371318Z","shell.execute_reply.started":"2022-12-20T17:16:32.364352Z","shell.execute_reply":"2022-12-20T17:16:32.370229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-two-one\"></a>\n### Improving Model 2\nDue to the wiggly line and low performance of model 2, we will try to improve the model by adding optional stopping and adam optimizer.\nBelow we first specify early stopping and define model architecture as always.","metadata":{}},{"cell_type":"code","source":"early_stopping = EarlyStopping(\n    min_delta = 0.001, # minimium amount of change to count as an improvement\n    patience = 5, # how many epochs to wait before stopping\n    restore_best_weights = True,\n)\n\nmodel2_2 = keras.Sequential([\n    vgg,\n    layers.Flatten(),\n    layers.Dense(units = 512, activation = \"relu\"),\n    layers.Dropout(rate = 0.3),\n    layers.Dense(units = 512, activation = \"relu\"),\n    layers.Dropout(rate = 0.3),\n    layers.Dense(1, activation = \"sigmoid\")\n])\nmodel2_2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:16:36.420901Z","iopub.execute_input":"2022-12-20T17:16:36.421287Z","iopub.status.idle":"2022-12-20T17:16:36.518569Z","shell.execute_reply.started":"2022-12-20T17:16:36.421227Z","shell.execute_reply":"2022-12-20T17:16:36.51748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train the model","metadata":{}},{"cell_type":"code","source":"model2_2.compile(\n    optimizer = \"adam\",\n    loss = \"binary_crossentropy\",\n    metrics = [\"accuracy\"]\n)\n\nhistory2_2 = model2_2.fit(\n    train_data,\n    train_labels,\n    validation_data = (val_data,val_labels),\n    batch_size = 64,\n    epochs = 25,\n    callbacks = [early_stopping],\n    verbose = 1\n)\n\nmodel2_2.save(\"Model2_2.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:16:40.942853Z","iopub.execute_input":"2022-12-20T17:16:40.943208Z","iopub.status.idle":"2022-12-20T17:18:22.079156Z","shell.execute_reply.started":"2022-12-20T17:16:40.943178Z","shell.execute_reply":"2022-12-20T17:18:22.078181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot","metadata":{}},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\nt = f.suptitle(\"Model 2.2 Performance\", fontsize = 12)\nf.subplots_adjust(top = 0.85, wspace = 0.3)\n\nmax_epoch = len(history2_2.history[\"accuracy\"])+1\nepoch_list = list(range(1, max_epoch))\nax1.plot(epoch_list, history2_2.history[\"accuracy\"], label = \"Train Accuracy\")\nax1.plot(epoch_list, history2_2.history[\"val_accuracy\"], label = \"Validation Accuracy\")\nax1.set_xticks(np.arange(1, max_epoch, 5))\nax1.set_yticks(np.arange(0.75, 1, 0.02))\nax1.set_ylabel(\"Accuracy Value\")\nax1.set_xlabel(\"Epoch\")\nax1.set_title(\"Accuracy\")\nl1 = ax1.legend(loc = 4)\n\nax2.plot(epoch_list, history2_2.history[\"loss\"], label = \"Train Loss\")\nax2.plot(epoch_list, history2_2.history[\"val_loss\"], label = \"Validation Loss\")\nax2.set_xticks(np.arange(1, max_epoch, 5))\nax2.set_yticks(np.arange(0.0, 0.5, 0.05))\nax2.set_ylabel(\"Loss Value\")\nax2.set_xlabel(\"Epoch\")\nax2.set_title(\"Loss\")\nl2 = ax2.legend(loc = \"best\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:18:29.380106Z","iopub.execute_input":"2022-12-20T17:18:29.380807Z","iopub.status.idle":"2022-12-20T17:18:29.72701Z","shell.execute_reply.started":"2022-12-20T17:18:29.38077Z","shell.execute_reply":"2022-12-20T17:18:29.726098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that the lines of the validation sets are a little more stabilized and the trend is in accordance with the training sets. The loss seemed to have reduced, showing that there is less potential for the model to be overfitted.","metadata":{}},{"cell_type":"code","source":"history2_2_frame = pd.DataFrame(history2_2.history)\nprint(\"Model 2.2 maximum validation accuracy: {}\".format(history2_2_frame[\"val_accuracy\"].max()))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:19:43.453858Z","iopub.execute_input":"2022-12-20T17:19:43.454272Z","iopub.status.idle":"2022-12-20T17:19:43.462744Z","shell.execute_reply.started":"2022-12-20T17:19:43.454219Z","shell.execute_reply":"2022-12-20T17:19:43.461563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-three\"></a>\n## 3.3 Pretrained Convolutional Network with Data Augumentation and Fine Tuning\n\n#### Model Architecture\nIn this section, we explore fine-tuning the weights of the layers of the pre-trained VGG-19 model and use image augmentation. We will be using `ImageDataGenerator` in `tf.keras` which helps to build image augmentors.","metadata":{}},{"cell_type":"code","source":"train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(zoom_range = 0.05, \n                                                                rotation_range = 25,\n                                                                width_shift_range = 0.05, \n                                                                height_shift_range = 0.05, \n                                                                shear_range = 0.05,\n                                                                horizontal_flip = True, \n                                                                fill_mode = \"nearest\")\n\nval_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n\n# configurations as in article\n# build image augmentation generators\ntrain_generator = train_datagen.flow(train_data, train_labels, batch_size = 64, shuffle = True)\nval_generator = val_datagen.flow(val_data, val_labels, batch_size = 64, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:19:47.699628Z","iopub.execute_input":"2022-12-20T17:19:47.699979Z","iopub.status.idle":"2022-12-20T17:19:47.708499Z","shell.execute_reply.started":"2022-12-20T17:19:47.699949Z","shell.execute_reply":"2022-12-20T17:19:47.70748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we look at the example image transformation, we see that there are variations in the image (slightly).","metadata":{}},{"cell_type":"code","source":"img_id = 0\nsample_generator = train_datagen.flow(train_data[img_id:img_id+1], train_labels[img_id:img_id+1],\n                                      batch_size = 1)\nsample = [next(sample_generator) for i in range(0,5)]\nfig, ax = plt.subplots(1,5, figsize = (16, 6))\nprint('Labels:', [item[1][0] for item in sample])\nl = [ax[i].imshow(sample[i][0][0]) for i in range(0,5)]","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:19:51.174617Z","iopub.execute_input":"2022-12-20T17:19:51.174967Z","iopub.status.idle":"2022-12-20T17:19:51.722287Z","shell.execute_reply.started":"2022-12-20T17:19:51.174938Z","shell.execute_reply":"2022-12-20T17:19:51.721398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, as in the article, we want to ensure that the last two blocks of the model are trainable:","metadata":{}},{"cell_type":"code","source":"vgg = tf.keras.applications.vgg19.VGG19(include_top = False, weights = \"imagenet\", \n                                        input_shape = input_shape)\n\n# Freeze the layers\nvgg.trainable = True\n\nset_trainable = False\nfor layer in vgg.layers:\n    if layer.name in [\"block5_conv1\", \"block4_conv1\"]:\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False\n\n# model based on article \nmodel3 = keras.Sequential([\n    vgg,\n    layers.Flatten(),\n    layers.Dense(units = 512, activation = \"relu\"),\n    layers.Dropout(rate = 0.3),\n    layers.Dense(units = 512, activation = \"relu\"),\n    layers.Dropout(rate = 0.3),\n    layers.Dense(1, activation = \"sigmoid\")\n])\nmodel3.summary()\n\nmodel3.compile(optimizer = tf.keras.optimizers.RMSprop(lr=1e-5),\n                loss = \"binary_crossentropy\",\n                metrics = [\"accuracy\"])\n\nprint(\"Total Layers:\", len(model3.layers))\nprint(\"Total trainable layers:\", sum([1 for l in model3.layers if l.trainable]))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:19:55.89413Z","iopub.execute_input":"2022-12-20T17:19:55.895106Z","iopub.status.idle":"2022-12-20T17:19:56.327199Z","shell.execute_reply.started":"2022-12-20T17:19:55.895059Z","shell.execute_reply":"2022-12-20T17:19:56.326302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train the model","metadata":{}},{"cell_type":"code","source":"train_steps_per_epoch = train_generator.n // train_generator.batch_size\nval_steps_per_epoch = val_generator.n // val_generator.batch_size\nhistory3 = model3.fit_generator(train_generator,\n                                steps_per_epoch = train_steps_per_epoch,\n                                epochs = 25,\n                                validation_data = val_generator,\n                                validation_steps = val_steps_per_epoch, \n                                callbacks = [early_stopping],\n                                verbose = 1)\n\n\nmodel3.save(\"Model3.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:20:01.057809Z","iopub.execute_input":"2022-12-20T17:20:01.058161Z","iopub.status.idle":"2022-12-20T17:23:44.135327Z","shell.execute_reply.started":"2022-12-20T17:20:01.058129Z","shell.execute_reply":"2022-12-20T17:23:44.134285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot","metadata":{}},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\nt = f.suptitle(\"Model 3 Performance\", fontsize = 12)\nf.subplots_adjust(top = 0.85, wspace = 0.3)\n\nmax_epoch = len(history3.history[\"accuracy\"])+1\nepoch_list = list(range(1, max_epoch))\nax1.plot(epoch_list, history3.history[\"accuracy\"], label = \"Train Accuracy\")\nax1.plot(epoch_list, history3.history[\"val_accuracy\"], label = \"Validation Accuracy\")\nax1.set_xticks(np.arange(1, max_epoch, 5))\nax1.set_yticks(np.arange(0.75, 1, 0.02))\nax1.set_ylabel(\"Accuracy Value\")\nax1.set_xlabel(\"Epoch\")\nax1.set_title(\"Accuracy\")\nl1 = ax1.legend(loc = 4)\n\nax2.plot(epoch_list, history3.history[\"loss\"], label = \"Train Loss\")\nax2.plot(epoch_list, history3.history[\"val_loss\"], label = \"Validation Loss\")\nax2.set_xticks(np.arange(1, max_epoch, 5))\nax2.set_yticks(np.arange(0.0, 0.5, 0.05))\nax2.set_ylabel(\"Loss Value\")\nax2.set_xlabel(\"Epoch\")\nax2.set_title(\"Loss\")\nl2 = ax2.legend(loc = \"best\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:23:44.137053Z","iopub.execute_input":"2022-12-20T17:23:44.137439Z","iopub.status.idle":"2022-12-20T17:23:48.479074Z","shell.execute_reply.started":"2022-12-20T17:23:44.1374Z","shell.execute_reply":"2022-12-20T17:23:48.47816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The performance seemed to increase as compared to the last model. Also, when looking at both accuracy and loss, we can observe that there is not a visible trend of overfitting occurring.","metadata":{}},{"cell_type":"code","source":"history3_frame = pd.DataFrame(history3.history)\nprint(\"Model 3 maximum validation accuracy: {}\".format(history3_frame[\"val_accuracy\"].max()))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:24:26.913399Z","iopub.execute_input":"2022-12-20T17:24:26.913772Z","iopub.status.idle":"2022-12-20T17:24:26.921751Z","shell.execute_reply.started":"2022-12-20T17:24:26.913741Z","shell.execute_reply":"2022-12-20T17:24:26.920472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-three-one\"></a>\n### Improving Model 3\nWe will try to improve the model by adding the adam optimizer, to check whether any tuning can help increase the accuracy.\nSpoiler alert: this did not help.","metadata":{}},{"cell_type":"code","source":"model3_2 = keras.Sequential([\n    vgg,\n    layers.Flatten(),\n    layers.Dense(units = 512, activation = \"relu\"),\n    layers.Dropout(rate = 0.3),\n    layers.Dense(units = 512, activation = \"relu\"),\n    layers.Dropout(rate = 0.3),\n    layers.Dense(1, activation = \"sigmoid\")\n])\nmodel3_2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:24:31.497415Z","iopub.execute_input":"2022-12-20T17:24:31.497769Z","iopub.status.idle":"2022-12-20T17:24:31.587941Z","shell.execute_reply.started":"2022-12-20T17:24:31.49774Z","shell.execute_reply":"2022-12-20T17:24:31.586893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3_2.compile(optimizer = \"adam\",\n                loss = \"binary_crossentropy\",\n                metrics = [\"accuracy\"])\n\nhistory3_2 = model3_2.fit_generator(\n    train_generator, \n    steps_per_epoch = train_steps_per_epoch, \n    epochs = 25,\n    validation_data = val_generator, \n    validation_steps = val_steps_per_epoch, \n    callbacks = [early_stopping],\n    verbose = 1)\n\nmodel3_2.save(\"Model3_2.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:24:36.245295Z","iopub.execute_input":"2022-12-20T17:24:36.245659Z","iopub.status.idle":"2022-12-20T17:31:43.147493Z","shell.execute_reply.started":"2022-12-20T17:24:36.245618Z","shell.execute_reply":"2022-12-20T17:31:43.146514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot","metadata":{}},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\nt = f.suptitle(\"Model 3.2 Performance\", fontsize = 12)\nf.subplots_adjust(top = 0.85, wspace = 0.3)\n\nmax_epoch = len(history3_2.history[\"accuracy\"])+1\nepoch_list = list(range(1, max_epoch))\nax1.plot(epoch_list, history3_2.history[\"accuracy\"], label = \"Train Accuracy\")\nax1.plot(epoch_list, history3_2.history[\"val_accuracy\"], label = \"Validation Accuracy\")\nax1.set_xticks(np.arange(1, max_epoch, 5))\nax1.set_yticks(np.arange(0.75, 1, 0.02))\nax1.set_ylabel(\"Accuracy Value\")\nax1.set_xlabel(\"Epoch\")\nax1.set_title(\"Accuracy\")\nl1 = ax1.legend(loc = 4)\n\nax2.plot(epoch_list, history3_2.history[\"loss\"], label = \"Train Loss\")\nax2.plot(epoch_list, history3_2.history[\"val_loss\"], label = \"Validation Loss\")\nax2.set_xticks(np.arange(1, max_epoch, 5))\nax2.set_yticks(np.arange(0.0, 0.5, 0.05))\nax2.set_ylabel(\"Loss Value\")\nax2.set_xlabel(\"Epoch\")\nax2.set_title(\"Loss\")\nl2 = ax2.legend(loc = \"best\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:31:43.149465Z","iopub.execute_input":"2022-12-20T17:31:43.149827Z","iopub.status.idle":"2022-12-20T17:31:47.275213Z","shell.execute_reply.started":"2022-12-20T17:31:43.149791Z","shell.execute_reply":"2022-12-20T17:31:47.274156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As it was already mentioned, the accuracy and loss seemed to be similar to what we observed in the previous model. This indicates that the adam optimizer did not improve the performance of the model. Furthermore, we see that the validation accuracy is quite wiggly and unstable. To conclude, these adjustments didn't improve the model.","metadata":{}},{"cell_type":"code","source":"history3_2_frame = pd.DataFrame(history3_2.history)\nprint(\"Model 3.2 maximum validation accuracy: {}\".format(history3_2_frame[\"val_accuracy\"].max()))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:31:58.792566Z","iopub.execute_input":"2022-12-20T17:31:58.792929Z","iopub.status.idle":"2022-12-20T17:31:58.799834Z","shell.execute_reply.started":"2022-12-20T17:31:58.7929Z","shell.execute_reply":"2022-12-20T17:31:58.798801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-four\"></a>\n## 3.4 Customized Convolutional Network\n\nFinally, based on all the models from the article and our attempt to tune the model, we customized the model to increase the overall performance.\n\n#### Attempts & Model Architecture\nWe tried multiple options and settings to see if we could increase the accuracy.\n- Multiple data augmentations and different settings for the data augmentation arguments\n- More/less convolutional layers\n- More/fewer filters\n- Bigger/smaller kernel sizes\n- Changing the padding\n- More/less hidden layers in the classifier head\n- More/fewer nodes in the hidden layers\n\nMost changes made didn't improve the accuracy so, in the end, we decided to keep the network simple. In this way, we keep the training time low and the accuracy is not really affected. We did however want to include a lot of data augmentation to account for possible variations in data.\n\n**In the end, we settled on these options:**\n- Data Augmentation\n    - Random Contrast (factor 0.05, 0.10, and 0.15)\n    - Random Flip (horizontal only as vertical flip drastically decreased the accuracy for some reason)\n    - Random Rotation (factor 0.05, 0.10, and 0.15)\n \n \n- 3 Convolutional layers and after every layer Max pooling\n    - Layer 1: 32 filters, kernel_size of 3, relu activation, and \"same\" padding.\n    - Layer 2: 64 filters, kernel_size of 3, relu activation, and \"same\" padding.\n    - Layer 3: 128 filters, kernel_size of 3, relu activation, and \"same\" padding.\n    \n\n- Classifier head with 2 hidden layers with each 10 units and both with the relu activation.\n\n- Include early stopping to prevent overfitting","metadata":{}},{"cell_type":"code","source":"model4 = keras.Sequential([\n   layers.InputLayer(input_shape = input_shape),\n    \n    # Data Augmentation\n    preprocessing.RandomContrast(factor = 0.05),\n    preprocessing.RandomContrast(factor = 0.10),\n    preprocessing.RandomContrast(factor = 0.15),\n    preprocessing.RandomFlip(mode = \"horizontal\"),\n    preprocessing.RandomRotation(factor = 0.05),\n    preprocessing.RandomRotation(factor = 0.10),\n    preprocessing.RandomRotation(factor = 0.15),\n    \n    # First Convolutional Block\n    layers.Conv2D(filters = 32, kernel_size = 3, activation = \"relu\", padding = \"same\"),\n    layers.MaxPool2D(),\n\n    # Second Convolutional Block\n    layers.Conv2D(filters = 64, kernel_size = 3, activation = \"relu\", padding = \"same\"),\n    layers.MaxPool2D(),\n\n    # Third Convolutional Block\n    layers.Conv2D(filters = 128, kernel_size = 3, activation = \"relu\", padding = \"same\"),\n    layers.MaxPool2D(),\n\n\n    # Classifier Head\n    layers.Flatten(),\n    layers.Dense(units = 10, activation = \"relu\"),\n    layers.Dense(units = 10, activation = \"relu\"),\n    layers.Dense(units = 1, activation = \"sigmoid\"),\n])\nmodel4.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:32:09.237026Z","iopub.execute_input":"2022-12-20T17:32:09.237399Z","iopub.status.idle":"2022-12-20T17:32:09.533571Z","shell.execute_reply.started":"2022-12-20T17:32:09.237368Z","shell.execute_reply":"2022-12-20T17:32:09.532667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train model\nSince this is a two-class problem, we used the binary versions of cross-entropy and accuracy. We also chose adam optimizer, as it generally performs well.","metadata":{}},{"cell_type":"code","source":"model4.compile(\n    optimizer = \"adam\",\n    loss = \"binary_crossentropy\",\n    metrics = [\"accuracy\"]\n)\n\nhistory4 = model4.fit(\n    train_data,\n    train_labels,\n    validation_data = (val_data,val_labels),\n    epochs = 40,\n    callbacks = [early_stopping],\n    verbose = 1\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:39:28.970745Z","iopub.execute_input":"2022-12-20T17:39:28.971088Z","iopub.status.idle":"2022-12-20T17:40:50.955486Z","shell.execute_reply.started":"2022-12-20T17:39:28.971059Z","shell.execute_reply":"2022-12-20T17:40:50.954564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot","metadata":{}},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\nt = f.suptitle(\"Model 4 Performance\", fontsize = 12)\nf.subplots_adjust(top = 0.85, wspace = 0.3)\n\nmax_epoch = len(history4.history[\"accuracy\"])+1\nepoch_list = list(range(1, max_epoch))\nax1.plot(epoch_list, history4.history[\"accuracy\"], label = \"Train Accuracy\")\nax1.plot(epoch_list, history4.history[\"val_accuracy\"], label = \"Validation Accuracy\")\nax1.set_xticks(np.arange(1, max_epoch, 5))\nax1.set_yticks(np.arange(0.75, 1, 0.02))\nax1.set_ylabel(\"Accuracy Value\")\nax1.set_xlabel(\"Epoch\")\nax1.set_title(\"Accuracy\")\nl1 = ax1.legend(loc = 4)\n\nax2.plot(epoch_list, history4.history[\"loss\"], label = \"Train Loss\")\nax2.plot(epoch_list, history4.history[\"val_loss\"], label = \"Validation Loss\")\nax2.set_xticks(np.arange(1, max_epoch, 5))\nax2.set_yticks(np.arange(0.0, 0.5, 0.05))\nax2.set_ylabel(\"Loss Value\")\nax2.set_xlabel(\"Epoch\")\nax2.set_title(\"Loss\")\nl2 = ax2.legend(loc = \"best\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:41:10.919038Z","iopub.execute_input":"2022-12-20T17:41:10.919415Z","iopub.status.idle":"2022-12-20T17:41:11.254211Z","shell.execute_reply.started":"2022-12-20T17:41:10.919384Z","shell.execute_reply":"2022-12-20T17:41:11.253151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This seems to be one of the best model performance we have. The validation accuracy seemed to peak among all four models, as well as having almost no loss for both training and validation sets.","metadata":{}},{"cell_type":"code","source":"history4_frame = pd.DataFrame(history4.history)\nprint(\"Model 4 maximum validation accuracy: {}\".format(history4_frame[\"val_accuracy\"].max()))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:42:33.687804Z","iopub.execute_input":"2022-12-20T17:42:33.688196Z","iopub.status.idle":"2022-12-20T17:42:33.697722Z","shell.execute_reply.started":"2022-12-20T17:42:33.688155Z","shell.execute_reply":"2022-12-20T17:42:33.69569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# 4. Result:\nNow, we will compare the models in the following step:\n1. Use all six models (main four and two fine-tunned models) to predict the test data \n2. Make confusion matrices and calculate the accuracy, precision, recall, and f1-score for every model.\n\nFor the confusion matrix, we used a function created by [Roi Polanitzer](https://medium.com/@polanitzer/building-a-convolutional-neural-network-in-python-predict-digits-from-gray-scale-images-of-550d79b358b) (Keep in mind that we made a few changes so the function accommodates our goal)\n","metadata":{}},{"cell_type":"code","source":"# Confusion matrix function\n\ndef plot_confusion_matrix(cm,\n                          classes,\n                          title = \"Confusion matrix\",\n                          cmap = plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    \"\"\"\n    plt.imshow(cm, interpolation = \"nearest\", cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 45)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max() / 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, \n                 cm[i, j],\n                 horizontalalignment = \"center\",\n                 color = \"white\" if cm[i, j] > thresh else \"black\")\n        \n    plt.tight_layout()\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:43:01.914231Z","iopub.execute_input":"2022-12-20T17:43:01.914666Z","iopub.status.idle":"2022-12-20T17:43:01.928768Z","shell.execute_reply.started":"2022-12-20T17:43:01.914627Z","shell.execute_reply":"2022-12-20T17:43:01.927511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix\nFirst, we will use the function to visualize the confusion matrices of every model, and we will explain what we can conclude from the matrices at the end of section 4.\n\n#### Model 1: Basic Convolutional Network from scratch","metadata":{}},{"cell_type":"code","source":"# Model 1\nmodel1 = load_model(\"Model1.h5\")\n\ny_pred_model1 = model1.predict(test_data)\ny_pred1 = np.round(y_pred_model1)\n\ncm_plot_labels = [\"Uninfected\", \"Infected\"]\ncm1 = skmet.confusion_matrix(y_true = test_labels, y_pred = y_pred1)\n\nplot_confusion_matrix(cm = cm1, classes = cm_plot_labels, title = \"Model 1\")\n","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:43:05.91376Z","iopub.execute_input":"2022-12-20T17:43:05.914183Z","iopub.status.idle":"2022-12-20T17:43:07.701999Z","shell.execute_reply.started":"2022-12-20T17:43:05.914146Z","shell.execute_reply":"2022-12-20T17:43:07.701091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model 2: Pretrained Convolutional Network (VGG-19)","metadata":{}},{"cell_type":"code","source":"# Model 2\nmodel2 = load_model(\"Model2.h5\")\n\ny_pred_model2 = model2.predict(test_data)\ny_pred2 = np.round(y_pred_model2)\n\ncm2 = skmet.confusion_matrix(y_true = test_labels, y_pred = y_pred2)\n\nplot_confusion_matrix(cm = cm2, classes = cm_plot_labels, title = \"Model 2\")\n","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:43:11.584817Z","iopub.execute_input":"2022-12-20T17:43:11.585179Z","iopub.status.idle":"2022-12-20T17:43:15.536194Z","shell.execute_reply.started":"2022-12-20T17:43:11.585148Z","shell.execute_reply":"2022-12-20T17:43:15.535261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model 2.2: Pretrained Convolutional Network (VGG-19) with Model Improvement","metadata":{}},{"cell_type":"code","source":"# Model 2.2\nmodel2_2 = load_model(\"Model2_2.h5\")\n\ny_pred_model2_2 = model2_2.predict(test_data)\ny_pred2_2 = np.round(y_pred_model2_2)\n\ncm2_2 = skmet.confusion_matrix(y_true = test_labels, y_pred = y_pred2_2)\n\nplot_confusion_matrix(cm = cm2_2, classes = cm_plot_labels, title = \"Model 2.2\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:43:28.686482Z","iopub.execute_input":"2022-12-20T17:43:28.686853Z","iopub.status.idle":"2022-12-20T17:43:32.414066Z","shell.execute_reply.started":"2022-12-20T17:43:28.686821Z","shell.execute_reply":"2022-12-20T17:43:32.413123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model 3: Pretrained Convolutional Network with Data Augumentation and Fine Tuning","metadata":{}},{"cell_type":"code","source":"# Model 3\nmodel3 = load_model(\"Model3.h5\")\n\ny_pred_model3 = model3.predict(test_data)\ny_pred3 = np.round(y_pred_model3)\n\ncm3 = skmet.confusion_matrix(y_true = test_labels, y_pred = y_pred3)\n\nplot_confusion_matrix(cm = cm3, classes = cm_plot_labels, title = \"Model 3\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:44:06.344716Z","iopub.execute_input":"2022-12-20T17:44:06.345082Z","iopub.status.idle":"2022-12-20T17:44:09.762224Z","shell.execute_reply.started":"2022-12-20T17:44:06.345049Z","shell.execute_reply":"2022-12-20T17:44:09.76135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model 3.2: Pretrained Convolutional Network with Data Augumentation and Fine Tuning & Model Improvement","metadata":{}},{"cell_type":"code","source":"# Model 3.2\nmodel3_2 = load_model(\"Model3_2.h5\")\n\ny_pred_model3_2 = model3_2.predict(test_data)\ny_pred3_2 = np.round(y_pred_model3_2)\n\ncm3_2 = skmet.confusion_matrix(y_true = test_labels, y_pred = y_pred3_2)\n\nplot_confusion_matrix(cm = cm3_2, classes = cm_plot_labels, title = \"Model 3.2\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T17:43:49.062754Z","iopub.execute_input":"2022-12-20T17:43:49.063101Z","iopub.status.idle":"2022-12-20T17:43:52.636204Z","shell.execute_reply.started":"2022-12-20T17:43:49.063069Z","shell.execute_reply":"2022-12-20T17:43:52.635155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model 4: Customized Convolutional Network","metadata":{}},{"cell_type":"code","source":"# Model 4\nmodel4 = load_model(\"Model4.h5\")\n\ny_pred_model4 = model4.predict(test_data)\ny_pred4 = np.round(y_pred_model4)\n\ncm4 = skmet.confusion_matrix(y_true = test_labels, y_pred = y_pred4)\n\nplot_confusion_matrix(cm = cm4, classes = cm_plot_labels, title = \"Model 4\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T18:47:22.650862Z","iopub.execute_input":"2022-12-20T18:47:22.651494Z","iopub.status.idle":"2022-12-20T18:47:24.444574Z","shell.execute_reply.started":"2022-12-20T18:47:22.651452Z","shell.execute_reply":"2022-12-20T18:47:24.443671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model comparison\nNow, we will combine the outputs of the confusion matrix of all models into one big data frame to visualize the difference between all models.","metadata":{}},{"cell_type":"code","source":"#Model 1\n\n# accuracy\naccuracy_m1 = skmet.accuracy_score(test_labels, y_pred1)\n# precision \nprecision_m1 = skmet.precision_score(test_labels, y_pred1)\n# recall\nrecall_m1 = skmet.recall_score(test_labels, y_pred1)\n# f1\nf1_m1 = skmet.f1_score(test_labels, y_pred1)\n\n\n#Model 2\n\n# accuracy\naccuracy_m2 = skmet.accuracy_score(test_labels, y_pred2)\n# precision \nprecision_m2 = skmet.precision_score(test_labels, y_pred2)\n# recall\nrecall_m2 = skmet.recall_score(test_labels, y_pred2)\n# f1\nf1_m2 = skmet.f1_score(test_labels, y_pred2)\n\n\n#Model 2.2\n\n# accuracy\naccuracy_m2_2 = skmet.accuracy_score(test_labels, y_pred2_2)\n# precision \nprecision_m2_2 = skmet.precision_score(test_labels, y_pred2_2)\n# recall\nrecall_m2_2 = skmet.recall_score(test_labels, y_pred2_2)\n# f1\nf1_m2_2 = skmet.f1_score(test_labels, y_pred2_2)\n\n\n#Model 3\n\n# accuracy\naccuracy_m3 = skmet.accuracy_score(test_labels, y_pred3)\n# precision \nprecision_m3 = skmet.precision_score(test_labels, y_pred3)\n# recall\nrecall_m3 = skmet.recall_score(test_labels, y_pred3)\n# f1\nf1_m3 = skmet.f1_score(test_labels, y_pred3)\n\n\n#Model 3.2\n\n# accuracy\naccuracy_m3_2 = skmet.accuracy_score(test_labels, y_pred3_2)\n# precision \nprecision_m3_2 = skmet.precision_score(test_labels, y_pred3_2)\n# recall\nrecall_m3_2 = skmet.recall_score(test_labels, y_pred3_2)\n# f1\nf1_m3_2 = skmet.f1_score(test_labels, y_pred3_2)\n\n\n#Model 4\n\n# accuracy\naccuracy_m4 = skmet.accuracy_score(test_labels, y_pred4)\n# precision \nprecision_m4 = skmet.precision_score(test_labels, y_pred4)\n# recall\nrecall_m4 = skmet.recall_score(test_labels, y_pred4)\n# f1\nf1_m4 = skmet.f1_score(test_labels, y_pred4)\n\n\n# Create dataframe\ndata = {\"Accuracy\": [accuracy_m1, accuracy_m2, accuracy_m2_2, accuracy_m3, accuracy_m3_2, accuracy_m4],\n        \"Precision\": [precision_m1, precision_m2, precision_m2_2, precision_m3, precision_m3_2, precision_m4],\n        \"Recall\": [recall_m1, recall_m2, recall_m2_2, recall_m3, recall_m3_2, recall_m4],\n        \"F1-score\": [f1_m1, f1_m2, f1_m2_2, f1_m3, f1_m3_2, f1_m4]\n       }\n \nmodel_comp = pd.DataFrame(data, index = [\"Model 1\", \"Model 2\", \"Model 2.2\", \"Model 3\", \"Model 3.2\", \"Model 4\"])\nmodel_comp","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:12:41.647641Z","iopub.execute_input":"2022-12-20T19:12:41.647997Z","iopub.status.idle":"2022-12-20T19:12:41.744043Z","shell.execute_reply.started":"2022-12-20T19:12:41.647967Z","shell.execute_reply":"2022-12-20T19:12:41.743045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-fourone\"></a>\n## Table interpretation\n### 1. Accuracy:\nAmong all the models, `Model 3` - Data augumented pre-trained model with fine tuning - appears to be the most accurate when predicting the test sets. It seems like `Model 1`, `Model 3.2` and `Model 4` also have similarly high accuracy scores.\nThe accuracy scores for the Basic CNN, Pretrained VGG-19, and VGG-19 Fine-tuned models in the article were: 0.9497, 0.9376, and 0.9600 respectively. Considering randomness and robustness in the splitted data and model fitting, we can assume that the results are successfully replicated.\n\n### 2. Precision:\nPrecision, again, is the highest for `Model 3` among all the models. This indicates that the number of uninfected images correctly predicted to belong in the uninfected images is 0.972. Similarly, the precision scores for `Model 1`, `Model 3.2` and `Model 4` were high as well.\n\n### 3. Recall / Sensitivity:\nThe recall score of `Model 3` is again the highest among all. Except for `Model 2` and `Model 2.2`, which have around 0.90 recall scores, every model seems to perform well as they all have recall scores higher than  0.93.\n\n### 4. F1-Score:\nAs F1 is the average score of precision and recall of the model, we again have `Model 3` as the best performing model. The high F1 score also indicates that we have low false positives and false negatives.\n\n### Robustness analysis:\nWhen comparing the accuracy scores with training data with test data, we observed that the scores approximately lie in a similar range of values. This indicates that the model performances do not vary a lot when using new sets of data vs. training data. However, this does not ensure that we have a permanent, reliable solution to malaria detection with ML, as we have assumed the data is not corrupted and organized as the provided data. More specific limitations will follow in [Conclusion & Discussion](#section-five).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# 5. Overall Conclusion & Discussion:\n\nOur project aimed to 1) build the best-performing algorithm to predict and classify the malaria parasite from thin blood smear images and 2) replicate the finding of Sakar (2019). We have used Basic Convolutional Network from scratch, Pretrained Convolutional Network (VGG-19), Pretrained Convolutional Network with Data Augmentation and Fine-Tuning, and Customized Convolutional Network, as well as two sub-models that focused on model improvement by adding several hyperparameters. Based on our confusion matrices with accuracy, precision, recall, and F1-score, we can conclude that **Model 3 - Pre-trained Convolutional Network Model with Data Augmentation and Fine-tuning** performs the best among all the models. Since we have split the data into training, validation, and test data, we have a high probability for the model to fit into new data successfully.\n\nHowever, our project also has several limitations. First, our data might not be generalizable to all populations with malaria parasites because we trained and tested our model solely on thin blood smear images, not other images. Second, the images we used were clean and organized – that there was no noise in the image – meaning it might not be the best representation of the real world's data, which is more corrupted and unorganized.\n\nDuring the project, our group noticed there might be some room for improvement on the model or the project as a whole. First, we can try different model configurations, for example, decreasing the learning rate, as this may allow the model to learn a more optimal set of weights. Second, we can use other datasets containing malaria-infected blood images to have more variations in the model. In addition, we can use images of other diseases to have the model differentiate malaria from other diseases. These will yield higher performance and better reflection of the real world.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n# References\n\nFeleke, D. G., Alemu, Y., & Yemanebirhane, N. (2021). Performance of rapid diagnostic tests, microscopy, loop-mediated isothermal amplification (LAMP) and PCR for malaria diagnosis in Ethiopia: a systematic review and meta-analysis. *Malaria Journal*, *20*(1), 1-11.\n\nPolanitzer, R. (2022, February 5). Building a convolutional neural network in python; predict digits from gray-scale images of... Medium. Retrieved December 19, 2022, from https://medium.com/@polanitzer/building-a-convolutional-neural-network-in-python-predict-digits-from-gray-scale-images-of-550d79b358b \n\nRajaraman, S., Antani, S. K., Poostchi, M., Silamut, K., Hossain, M. A., Maude, R. J., ... & Thoma, G. R. (2018). Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images. *PeerJ*, *6*, e4568.\n\nSarkar, D. (2019). *Detecting malaria with Deep Learning*. Medium. Retrieved from https://towardsdatascience.com/detecting-malaria-with-deep-learning-9e45c1e34b60 ","metadata":{}}]}