{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n## Data considerations\n\nThis project aims to build an algorithm to detect emotions from images with four different kinds of facial expressions: anger, disgust, happiness, and sadness. Before diving into the analysis, we would like to give an overview of how the data was collected: \n\n**1. Where do the data come from? (To which population will results generalize?)**\n* The data comes from the expanded Cohn-Kanada (CK+) database, which is released to promote research on detecting facial expressions (Lucey, et al., 2010; p. 1). The databse contains 101 pictures of participants, where the age range is 18 to 50, and 69% female, 81%, Euro-American, 13% Afro-American, and 6% other groups (Lucey, et al., 2010; p. 1).\n* As can be seen, the data is only generalizable to Euro/ American enthnicity, meaning other groups are not represented well by the data. Furthermore, the participants for the study were composed of actors or actresses, meaning they would fake their facial expressions to look more extreme than in real life. \n\n**2. What are candidate machine learning methods? (models? features?)**\n* As the problem issues binary outcomes, we can classify this problem as a binary classification problem. The  candidates are:\n<div style=\"display:flex; flex-direction: row; flex-wrap: nowrap; align-items: stretch; width:100%;\">\n    <div style=\"display:inline-block;width:45%;\">\n        <ul>\n<h5> Model Candidates: </h5>          \n<li> LDA\n<li> QDA\n<li> KNN \n<li> LASSO and RIDGE\n<li> Support Vector Machines\n<li> Tree and Random Forest\n        </ul>\n    </div>\n    <div style=\"display:inline-block;width:45%;\">\n        <ul>\n<h5> Feature Candidates: </h5>  \n<li> Spectral Features from Histogram Analysis\n<li> Edge Features Using Frey Slate Functions\n<li> Histogram related gradients and other features\n<li> Bag of features - which we did not explore\n        </ul>\n    </div>","metadata":{}},{"cell_type":"markdown","source":"**3. What is the Bayes' error bound? (Any guestimate from scientific literature or web resources?)**\n* As always, we assum humans are good at recognizing facial expressions. Although in reality, some facial expressions could be very subtle. \n* To have an idea of a lower bound on the Bayes bound (i.e., the minimum accuracy that should be achievable). The best 'machine' we have at hand to recognize emotion from facial expression in the human brain. How often do human judges get it correct? In a study by Mollahosseini et al. (2018) an estimate for human classification inter-rater agreement was obtained for 11 emotions. For the four included in this data set they are:\n\n\n| disgust  |  anger  |  happy  |  sad  |\n|---------:|--------:|--------:|-------|\n|  67.6%   | 62.3%   | 79.6%   | 69.7% |\n\n\nKeep this in mind when evaluating the performance of the classifiers that you'll train.\n\nAs always, it's handy to evaluate how the algorithm does on the training set: If the training set is not classified accurately, how can you expect the test set to do any better. This obvious fact is often overlookedâ€”surprisingly.\n\n--------------------------------","metadata":{}},{"cell_type":"markdown","source":"# 2. Packages and importing data\n\nIn this step, we download all the necessary packages and import dataset for analysis.","metadata":{}},{"cell_type":"code","source":"## Importing packages\n\nlibrary(tidyverse) # metapackage with lots of helpful functions\nlibrary(png) # package that can be used to read png image files in a simple format\nlibrary(caret)\n\n## Reading in files\n\n# You can access files the \"../input/\" directory.\n# You can see the files by running  \n\nlist.files(path = \"../input/\")","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2022-10-19T21:35:19.408549Z","iopub.execute_input":"2022-10-19T21:35:19.444263Z","iopub.status.idle":"2022-10-19T21:35:22.344097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the availabe directories\ndirs = dir(\"../input\", pattern=\"[^g]$\", recursive=TRUE, include.dirs = TRUE, full.names = TRUE)\ndirs","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:35:22.346544Z","iopub.execute_input":"2022-10-19T21:35:22.347912Z","iopub.status.idle":"2022-10-19T21:35:23.280098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all image files: file names ending \".png\" \nanger   = dir(grep(\"anger\",   dirs, value = TRUE), pattern = \"png$\", full.names = TRUE)\ndisgust = dir(grep(\"disgust\", dirs, value = TRUE), pattern = \"png$\", full.names = TRUE)\nhappy   = dir(grep(\"happy\",   dirs, value = TRUE), pattern = \"png$\", full.names = TRUE)\nsad     = dir(grep(\"sad\",     dirs, value = TRUE), pattern = \"png$\", full.names = TRUE)\ntest_im = dir(grep(\"test\",    dirs, value = TRUE), pattern = \"png$\", full.names = TRUE)\n\nstr(anger)\nstr(disgust)\nstr(happy)\nstr(sad)\nstr(test_im)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:35:23.282595Z","iopub.execute_input":"2022-10-19T21:35:23.283979Z","iopub.status.idle":"2022-10-19T21:35:23.39114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ok = file.copy(  happy[60], \"happy.png\", overwrite = TRUE)\nok = file.copy(    sad[61],   \"sad.png\", overwrite = TRUE)\nok = file.copy(  anger[61], \"anger.png\", overwrite = TRUE)\nok = file.copy(disgust[61], \"disgust.png\", overwrite = TRUE)\nIRdisplay::display_html('<img src=\"happy.png\" width=\"200\" style=\"float:left\" /><img src=\"sad.png\" width=\"200\" style=\"float:left\" /><img src=\"anger.png\" width=\"200\" style=\"float:left\" /><img src=\"disgust.png\" width=\"200\" style=\"float:left\" />')","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:35:23.393626Z","iopub.execute_input":"2022-10-19T21:35:23.394977Z","iopub.status.idle":"2022-10-19T21:35:23.434797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly the first is a happy face, but is the second a sad face, an angry face, or both?","metadata":{}},{"cell_type":"markdown","source":"<div style=color:darkblue;background-color:#fafaff;min-height:8em; >\n\n\n<br>\n<em> The second one seems like a sad face, third is angry, and the last one is disgust.\n    One of the characteristics of sad face is having the inner corners of eyebrows raised, eyelids loose, and lip corners pulled down. Thus, we cna conlcude the first one to be the sad face. For the third picture, we can see the eyebrows being pulled down, lower eyelids pulled up,and lips tightened. This can show that she is angry. For the last one, the characteristic of wrinkled nose shows that she is closer to disgust than other emotions.</em>\n<br>\n<br>\n    \n<!-- Use Markdown or HTML to format your answer -->\n    \n\n</div>","metadata":{}},{"cell_type":"markdown","source":"--------------------------------","metadata":{}},{"cell_type":"markdown","source":"\nWhen working with image data, you often have many more Gigabytes of raw data than you have RAM memory available. Therefore, it is often not possible to work with all data \"in memory\". Resizing images often helps, but may cause loss of information.\n\nThe images for this competition are\n\n- gray scale, so we need only one *color channel* \n- are only 48 by 48 pixels\n\nFurthermore there are only 2538 pictures in the training set. Therefore, we are lucky enough to be able to retain all images in RAM, and don't have to do \"special stuff\" to handle reading in image files while fitting a model.","metadata":{}},{"cell_type":"markdown","source":"Reading in images pixelwise is easiest: We simply store each image as a long vector of pixel intensities, row by row. Also we will need a vector that contains the emotion label for each of the images.","metadata":{}},{"cell_type":"code","source":"# Combine all filenames into a single vector\ntrain_image_files = c(anger, happy, sad, disgust)\n\n# Read in the images as pixel values (discarding color channels)\nX_train = sapply(train_image_files, function(nm) c(readPNG(nm)[,,1])) %>% t() \ny = c(rep(\"anger\", length(anger)), rep(\"happy\", length(happy)), rep(\"sad\", length(sad)), rep(\"disgust\", length(disgust)))\n\nX_test = sapply(test_im, function(nm) c(readPNG(nm)[,,1])) %>% t() \n\n\n# Change row and column names of X to something more managable\nrownames(X_train)      = gsub(\".+train/\", \"\", rownames(X_train))\nrownames(X_test) = gsub(\".+test/\",  \"\", rownames(X_test))\n\ncolnames(X_train) = colnames(X_test) = paste(\"p\",1:ncol(X_train), sep=\"\")\n\n# Check result (are X_train, X_test, and y what we expect)\nX_train[1:6,20:23] %>% print\ntable(y)\n                \nX_test[1:6,20:23] %>% print","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:35:23.437379Z","iopub.execute_input":"2022-10-19T21:35:23.438783Z","iopub.status.idle":"2022-10-19T21:35:39.711917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine X_train and X_test for the full data\nX_combined <- rbind(X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:35:39.714175Z","iopub.execute_input":"2022-10-19T21:35:39.715488Z","iopub.status.idle":"2022-10-19T21:35:40.259055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization utility function\nas_image = function(x, nr=sqrt(length(x))) {opar=par(mar=rep(0,4)); on.exit(par(opar)); image(t(matrix(x,nr))[,nr:1], col = gray(0:255/255),axes=F)}\n\n\noptions(repr.plot.width=4, repr.plot.height=4)\nas_image(X_train[13,])\nas_image(X_test[13,])","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:35:40.261408Z","iopub.execute_input":"2022-10-19T21:35:40.262738Z","iopub.status.idle":"2022-10-19T21:35:40.549462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------------------------------","metadata":{}},{"cell_type":"markdown","source":"# 3. Feature extraction\n## 3.1. Spectral Features from Histogram\n\n**DATA PREPARATION:**\n\nChanging data frame to one long tibble data frame for combining the computed spectral features:","metadata":{}},{"cell_type":"code","source":"data_prep <- as_tibble(X_combined, rownames = \"id\") %>%\n    select(id, everything())\ncombined_data <- data_prep %>%\n    pivot_longer(-id, names_to = \"p_pos\", values_to = \"p_val\")\ncombined_data %>%\n    head()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:35:40.55294Z","iopub.execute_input":"2022-10-19T21:35:40.554517Z","iopub.status.idle":"2022-10-19T21:35:42.118459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**FEATURE EXPLANATION**\n\n(Most of them from competition 2):\nWe added the following features:\n* **_Level_**: It simply indicate the mean across the samples of a signal. It can inform the changes over time by detecting the dynamics in mean signals.\n* **_Power_**: It represents the average squared amplitude of the signal. Power reflects the loudness of the signal, indicating the deviation from an absolute zero of X. The loudness may differ in different activities, as the strength of each activty differs. \n* **_Energy_**: It is the total power of the n samples. It may aid on distingushing between activities as the power of the signals all differ per activity. \n* **_Entropy_**: It interprets the average surprise of an observed value from histogram. The differences show by how low or high the probabililties are: the higher the probability, the less 'surprised' you are. \n\n* Based on characteristics of histogram, we extracted statistical features as well; giving information about their location of the distribution and width of the distribution:\n    - _Mean_, _Standard Deivations_, _Quantiles_, _Skewness_, _Median_, _Mode_, _Maximum and Minimum_, _Kurtosis_\n    - _Standard Error_, _Root Mean Squared_\n\n* Based on the frequency of the band, we used signal spectruem to compare different facial expressions:\n     - **_Mean frequency_**: We used general mean frequency to find which facial expression has higher mean frequency and what not. \n     - **_Spectral peak, spectral mean frequency, spectral standard deviation, spectral entropy_**:\n    - The spectrum of a signal shows which frequencies a signal is composed of. It shows the number of fluctuations, including signals like a walking patterns ... etc. Peaks show the strength in which peaks are presented. Notice that height of each frequency is the square of a linear regression coefficient, meaning we can use to predict the signals from a wave. These spectral features can be used as a probability density function as well (ex, mean frequency). \n    \n* **_Convolutions_**: For pattern recognition we used convolution. Convolution is a filter that passes over for example an image and extracting features that show a commonolatity in the images such that if the image has certain features, it belongs to a particular class. Thus, aiding in detecting facial expression. \n* **_Coefficient variance_**: It is the spread of the values in the epoch. The differences in values can be an indication of which signals in each epoch are different per facial expressions. \n* **_Width/heigth ratio_**: It is the ratio of width and height of the wave. This is an unstandardized method, which we aim to learn the differences in proportion of width and heigh per facial expression. \n* **_Magnitude Pixel Area_**: It is the absolute sum of the pixels. The sum of each pixels can aid differentiating different facial expressions, as the absolute sums are all different.\n\n**FUNCTION CREATION:**\n\nFeatures function are created to aid the calculation simplification. Below are the functions listed in the order of time domain, statistical, frequency, and other features.","metadata":{}},{"cell_type":"code","source":"# Helper functions\nmost_common_value = function(x) {\n    counts = table(x, useNA='no')\n    most_frequent = which.max(counts)\n    return(names(most_frequent))\n}\n\n# Level function\nlev_fun <- function(x){\n    lev = (1/length(x)) * sum(x)\n    return(lev)\n}\n\n# Energy function\nenergy_fun = function(x) {\n    ener = sum((x - mean(x))^2) \n    return(ener)\n}\n\n# Entropy function\nent_fun  <- function(x, nbreaks = nclass.Sturges(x)) {\n    r = range(x)\n    x_binned = findInterval(x, seq(r[1], r[2], len= nbreaks))\n    h = tabulate(x_binned, nbins = nbreaks) # fast histogram\n    p = h/sum(h)\n    -sum(p[p>0] * log(p[p>0]))\n}\n\n# Interquartile Range function\niqr_fun <- function(x) {\n    iqr = quantile(x, .75) - quantile(x, .25)\n    return(iqr)\n}\n\n# Mode function\nmode_fun <- function(x) {\n    mode = as.integer(which.max(table(x)))\n    return(mode)   \n}\n\n# Standard Error function\nse_fun <- function(x) {\n    se = sd(x) / sqrt(length(x))\n    return(se)\n}\n\n# Root Mean Square function\nrms_fun <- function(x){ #only on vectors\n    rms = sqrt((1/length(x)*(sum(x^2))))\n    return (rms)\n}\n\n# Mean Frequency function\nmean_freq <- function(x) {\n    out_1 = numeric()\n    for (i in 1:length(x)){\n        out_1[i] = i * x[i] }\n        return(sum(out_1)/sum(x))\n}\n\n#Spectral Peak function\nspec_peak = function(x) {\n    spec = spectrum(x, log = 'n', plot = FALSE)\n    peak = spec$freq[which.max(spec$spec)]\n    return(peak)\n}\n\n# Spectral Mean Frequency function\nspec_mean_freq <- function(x) {\n    spec = spectrum(x, log = 'n', plot = FALSE)\n    df = spec$freq[2] - spec$freq[1]\n    sbar = sum(spec$freq * spec$spec * df)\n    return(sbar)\n}\n\n# Sepctral Standard Deviation function\nspec_sd <- function(x) {\n    spec = spectrum(x, log = 'n', plot = FALSE)\n    df = spec$freq[2] - spec$freq[1]\n    svar = sum((spec$freq - mean(x))^2 * spec$spec * df)\n    return(svar)\n}\n\n# Spectral Entropy function\nspec_ent <- function(x) {\n  spec = spectrum(x, log='n', plot = FALSE)$spec\n  ent_fun(spec)\n}\n\n# Magnitude Pixel Area\nmpa_fun <- function(s){\n    mpa <- sum(abs(s))\n    return(mpa)\n}\n\n# Pattern detection by convolutional neural networks function\nconv_fun <- function(x, m) {\n    w = 1/(2*m+1)\n    conv = max(stats::filter(x, w, method = 'convolution'))\n    return(conv)\n}","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:35:42.12086Z","iopub.execute_input":"2022-10-19T21:35:42.122304Z","iopub.status.idle":"2022-10-19T21:35:42.16315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**APPLICATION OF FUNCTIONS:**\n\nCreate a data frame for all the spectral features","metadata":{}},{"cell_type":"code","source":"specf_combined <- function(values) {\n    values %>%\n        group_by(id) %>%\n        summarize(\n            # Level\n            level_val = lev_fun(p_val),\n            \n            # Power\n            power_val = mean(p_val^2),\n            \n            # Energy\n            energy_val = energy_fun(p_val),\n            \n            # Entropy\n            ent_val = ent_fun(p_val),\n            \n            # Mean\n            mean_val = mean(p_val),\n            \n            # Standard deviation\n            sd_val = sd(p_val),\n            \n            # Quantiles\n            q_25_val = quantile(p_val, .25),\n            q_75_val = quantile(p_val, .75),\n            \n            # Median absolute deviation\n            mad_val = mad(p_val),\n            \n            # Mode\n            mod_val = mode_fun(p_val),\n            \n            # Min and Max\n            max_val = max(p_val),\n            mix_val = min(p_val),\n            \n            # Kurtosis\n            kur_val = e1071::kurtosis(p_val),\n            \n            # Standard Error\n            se_val = se_fun(p_val),\n            \n            # Root Mean Square\n            rms_val = rms_fun(p_val),\n            \n            # Coefficient variance\n            coefvar_val = sd_val/mean_val,\n            \n            # Mean Frequency\n            mfreq_val = mean_freq(p_val),\n            \n            # Spectral functions\n            peak_val = spec_peak(p_val),\n            s_meanfreq_val = spec_mean_freq(p_val),\n            s_sd_val = spec_sd(p_val),\n            s_ent_val = spec_ent(p_val),\n            \n            # Magnitude Pixel Area\n            mpa_val = mpa_fun(p_val),\n            \n            # Convolutional Network\n            conv_val = conv_fun(p_val, length(id)),\n            \n            # Witdh - Height Ratio\n            whratio_val = p_val[which.max(p_val)]/length(id),\n            \n            # Difference in max and min of values\n            diff_pv_val = max(p_val) - min(p_val)                 \n        )\n}\n\n# Apply spectral features to combined data\nspec_df <- specf_combined(combined_data)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:35:42.165637Z","iopub.execute_input":"2022-10-19T21:35:42.167079Z","iopub.status.idle":"2022-10-19T21:36:11.521109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's print out to see how the data frame looks like:","metadata":{}},{"cell_type":"code","source":"spec_df %>%\n    head()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:36:11.523572Z","iopub.execute_input":"2022-10-19T21:36:11.525004Z","iopub.status.idle":"2022-10-19T21:36:11.557844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Frey-Slate Features","metadata":{}},{"cell_type":"code","source":"options(repr.plot.width=4*4, repr.plot.height=4)\n\n# Compute edges by differencing neighboring pixels\nim = matrix(X_train[756,],48)\nh_edge = im[-1,] - im[-48,] # horizontal\nv_edge = im[,-1] - im[,-48] # vertical\nd_edge = h_edge[,-1] - h_edge[,-48] # diagonal\n\n# Specify a threshold (hand tuned here on visual result)\nthreshold = .0625 \n\nlayout(t(1:4))\nas_image(im)\nas_image(h_edge < threshold,   47); mtext(\"horizontal edge pixels\")\nas_image(v_edge < threshold,   48); mtext(\"vertical edge pixels\")\nas_image(d_edge < threshold/2, 47); mtext(\"diagonal edge pixels\")\n# as_image((h_edge[,-1] < 0.1) & (v_edge[-1,] < 0.1), 47); mtext(\"edge pixels\")","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:36:11.560328Z","iopub.execute_input":"2022-10-19T21:36:11.561964Z","iopub.status.idle":"2022-10-19T21:36:11.710561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load FreySlateFeatures function \nsource(\"https://bit.ly/32um24j\")\n\nFreySlateFeatures(h_edge < threshold)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:36:11.713204Z","iopub.execute_input":"2022-10-19T21:36:11.714646Z","iopub.status.idle":"2022-10-19T21:36:13.48793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the edges and frey-slate features are loaded, we can create empty matrices to store frey slate features","metadata":{}},{"cell_type":"code","source":"# Create three empty matrices to store the 16 Frey & Slate features\nfreyslate_h <- matrix(nrow = nrow(X_combined), ncol = 16)\nfreyslate_v <- matrix(nrow = nrow(X_combined), ncol = 16)\nfreyslate_d <- matrix(nrow = nrow(X_combined), ncol = 16)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:36:13.490369Z","iopub.execute_input":"2022-10-19T21:36:13.491802Z","iopub.status.idle":"2022-10-19T21:36:13.50803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We created the three matrices, but they lack column names, so we'll have to add them manually.","metadata":{}},{"cell_type":"code","source":"# Right now there are no column names, we have to make sure they are correct\ncolnames(freyslate_h)[1:16] <- paste0(names(FreySlateFeatures(h_edge < threshold)), \"_h\")\ncolnames(freyslate_v)[1:16] <- paste0(names(FreySlateFeatures(v_edge < threshold)), \"_v\")\ncolnames(freyslate_d)[1:16] <- paste0(names(FreySlateFeatures(d_edge < threshold)), \"_d\")\n\nhead(freyslate_h)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:36:13.510524Z","iopub.execute_input":"2022-10-19T21:36:13.511947Z","iopub.status.idle":"2022-10-19T21:36:13.543575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good. Now that the matrices are ready, we can construct and insert the features:","metadata":{}},{"cell_type":"code","source":"# Create a for loop to construct the 16 Frey & Slate features\nthreshold = .0625 \n\nfor (i in 1:nrow(X_combined)) {\n    \n    im = matrix(X_combined[i,],48)\n    \n    h_edge = im[-1,] - im[-48,]\n    v_edge = im[,-1] - im[,-48]\n    d_edge = h_edge[,-1] - h_edge[,-48]\n    \n    # Frey & Slate features, 16 per orientation (horizontal, vertical and diagonal)\n    freyslate_h[i,] <- FreySlateFeatures(h_edge < threshold)\n    freyslate_v[i,] <- FreySlateFeatures(v_edge < threshold)\n    freyslate_d[i,] <- FreySlateFeatures(d_edge < threshold)\n    \n}\n\nhead(freyslate_h)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:36:13.545898Z","iopub.execute_input":"2022-10-19T21:36:13.547332Z","iopub.status.idle":"2022-10-19T21:36:21.303357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks good, but some features have the exact same values per picture, which is a problem they hold little information. We will address this in the data cleaning section. For now, let's merge the two matrices into a single dataframe so that they're ready for further steps. We also need to add an \"id\" column for merging later.","metadata":{}},{"cell_type":"code","source":"# Use cbind() to merge the two matrices\nfreyslate_features <- cbind(freyslate_h,\n                            freyslate_v,\n                            freyslate_d)\n\n# An \"id\" column is still missing, which is needed to merge multiple features dataframes\nX2 <- X_combined %>% as_tibble(rownames = \"id\")\nX3 <- X2[,1]\nfreyslate_df <- cbind(X3, freyslate_features)\n\nfreyslate_df %>%\n    head()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:36:21.305681Z","iopub.execute_input":"2022-10-19T21:36:21.307014Z","iopub.status.idle":"2022-10-19T21:36:21.375491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3. Histogram features from edges using Frey-Slate Functions\nHistogram features work best on edges. An edge is a rapid change in pixel intensities, so if we compute the difference between two consecutive pixels, and check if it is larger than a certain threshold, we can find the pixels that are at the edge of an abrupt intensity change. We can use horizontal and vertical edges respectively. By computing differences in both north and west direction consecutively we filter out pixels that are part of a diagonal edge. \nNext up, we will create 4 different Histogram features, they are the mean, skewness, kurtosis and standard deviation. After that that we will create 16 different Frey & Slate features.\n\nYou can use the detected edge pixels to compute Frey and Slate type features: Histogram descriptives of the x and y locations of the 'on' pixels in the edge pixel maps.","metadata":{}},{"cell_type":"code","source":"# Create a for loop to construct the Histogram features\nthreshold = .0625 \n\nmean_horizontal <- mean_vertical <- mean_diagonal <- numeric(nrow(X_combined))\n\nskew_horizontal <- skew_vertical <- skew_diagonal <- numeric(nrow(X_combined))\n\nkurt_horizontal <- kurt_vertical <- kurt_diagonal <- numeric(nrow(X_combined))\n\nsd_horizontal <- sd_vertical <- sd_diagonal <- numeric(nrow(X_combined))\n\nsum_horizontal <- sum_vertical <- sum_diagonal <- numeric(nrow(X_combined))\n\nlev_horizontal <- lev_vertical <- lev_diagonal <- numeric(nrow(X_combined))\n\nenergy_horizontal <- energy_vertical <- energy_diagonal <- numeric(nrow(X_combined))\n\nspec_peak_horizontal <- spec_peak_vertical <- spec_peak_diagonal <- numeric(nrow(X_combined))\n\nspec_meanfreq_horizontal <- spec_meanfreq_vertical <- spec_meanfreq_diagonal <- numeric(nrow(X_combined))\n\nspec_sd_horizontal <- spec_sd_vertical <- spec_sd_diagonal <- numeric(nrow(X_combined))\n\nspec_ent_horizontal <- spec_ent_vertical <- spec_ent_diagonal <- numeric(nrow(X_combined))\n\nmpa_horizontal <- mpa_vertical <- mpa_diagonal <- numeric(nrow(X_combined))\n\nfor (i in 1:nrow(X_combined)) {\n    \n    im = matrix(X_combined[i,],48)\n    \n    h_edge = im[-1,] - im[-48,]\n    v_edge = im[,-1] - im[,-48]\n    d_edge = h_edge[,-1] - h_edge[,-48]\n    \n    # Mean\n    mean_horizontal[i] <- mean(h_edge < threshold)\n    mean_vertical[i] <- mean(v_edge < threshold)\n    mean_diagonal[i] <- mean(d_edge < threshold)\n    \n    # Skewness\n    skew_horizontal[i] <- e1071::skewness(h_edge < threshold)\n    skew_vertical[i] <- e1071::skewness(v_edge < threshold)\n    skew_diagonal[i] <- e1071::skewness(d_edge < threshold)\n\n    # Kurtosis\n    kurt_horizontal[i] <- e1071::kurtosis(h_edge < threshold)\n    kurt_vertical[i] <- e1071::kurtosis(v_edge < threshold)\n    kurt_diagonal[i] <- e1071::kurtosis(d_edge < threshold)\n\n    # Standard deviation\n    sd_horizontal[i] <- sd(h_edge < threshold)\n    sd_vertical[i] <- sd(v_edge < threshold)\n    sd_diagonal[i] <- sd(d_edge < threshold)\n    \n    # Sum\n    sum_horizontal[i] <- sum(h_edge < threshold)\n    sum_vertical[i] <- sum(v_edge < threshold)\n    sum_diagonal[i] <- sum(d_edge < threshold)\n\n    # Level\n    lev_horizontal[i] <- lev_fun(h_edge < threshold)\n    lev_vertical[i] <- lev_fun(v_edge < threshold)\n    lev_diagonal[i] <- lev_fun(d_edge < threshold)\n\n    # Energy\n    energy_horizontal[i] <- energy_fun(h_edge < threshold)\n    energy_vertical[i] <- energy_fun(v_edge < threshold)\n    energy_diagonal[i] <- energy_fun(d_edge < threshold)\n\n    # Sepctral peak\n    spec_peak_horizontal[i] <- spec_peak(h_edge < threshold)\n    spec_peak_vertical[i] <- spec_peak(v_edge < threshold)\n    spec_peak_diagonal[i] <- spec_peak(d_edge < threshold)\n\n    # Spectral Mean Frequency\n    spec_meanfreq_horizontal[i] <- spec_mean_freq(h_edge < threshold)\n    spec_meanfreq_vertical[i] <- spec_mean_freq(v_edge < threshold)\n    spec_meanfreq_diagonal[i] <- spec_mean_freq(d_edge < threshold)\n\n    # Spectral Standard Deviations\n    spec_sd_horizontal[i] <- spec_sd(h_edge < threshold)\n    spec_sd_vertical[i] <- spec_sd(v_edge < threshold)\n    spec_sd_diagonal[i] <- spec_sd(d_edge < threshold)\n\n    # Spectral Entropy\n    spec_ent_horizontal[i] <- spec_ent(h_edge < threshold)\n    spec_ent_vertical[i] <- spec_ent(v_edge < threshold)\n    spec_ent_diagonal[i] <- spec_ent(d_edge < threshold)\n\n    # Magnitude Pixel Area\n    mpa_horizontal[i] <- mpa_fun(h_edge < threshold)\n    mpa_vertical[i] <- mpa_fun(v_edge < threshold)\n    mpa_diagonal[i] <- mpa_fun(d_edge < threshold)\n    \n}\n\n# Combine all of the features into one data frame\nhistogram_df <- cbind(mean_horizontal, mean_vertical, mean_diagonal,\n                      skew_horizontal, skew_vertical, skew_diagonal,\n                      kurt_horizontal, kurt_vertical, kurt_diagonal,\n                      sd_horizontal, sd_vertical, sd_diagonal,\n                     sum_horizontal, sum_vertical, sum_diagonal,\n                     lev_horizontal, lev_vertical, lev_diagonal,\n                     energy_horizontal, energy_vertical, energy_diagonal,\n                     spec_peak_horizontal, spec_peak_vertical, spec_peak_diagonal,\n                     spec_meanfreq_horizontal, spec_meanfreq_vertical, spec_meanfreq_diagonal,\n                     spec_sd_horizontal, spec_sd_vertical, spec_sd_diagonal,\n                     spec_ent_horizontal, spec_ent_vertical, spec_ent_diagonal,\n                     mpa_horizontal, mpa_vertical, mpa_diagonal) %>%\n                as.data.frame()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T21:36:21.378057Z","iopub.execute_input":"2022-10-19T21:36:21.379408Z","iopub.status.idle":"2022-10-19T22:01:45.028187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks great, last step is to add an id column for merging with other frames later on.","metadata":{}},{"cell_type":"code","source":"# An \"id\" column is still missing, which is needed to merge multiple features dataframes. Make sure to only run this chunk once.\nX2 <- X_combined %>% as_tibble(rownames = \"id\")\nX3 <- X2[,1]\n\nhistogram_df <- cbind(X3, histogram_df)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:01:45.030562Z","iopub.execute_input":"2022-10-19T22:01:45.03202Z","iopub.status.idle":"2022-10-19T22:01:45.128315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take a look\nhistogram_df %>%\n    head()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:01:45.130778Z","iopub.execute_input":"2022-10-19T22:01:45.132173Z","iopub.status.idle":"2022-10-19T22:01:45.185943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4. Histogram of Oriented Gradients (HOG) features\n\nThis feature was adapted from group 1 and group 5, who were inspired by students from [last competition](https://www.kaggle.com/code/thomassvisser/can-we-predict-the-right-facial-expression-round-2/notebook?scriptVersionId=77717791).\n\nAccording to this [website](https://towardsdatascience.com/hog-histogram-of-oriented-gradients-67ecd887675f), the `histogram of oriented gradients (HOG)` is used in image processing to detect objects. These features count occurrences of gradient orientation in the localized portion of an image, by focusing on structuure or the shape of the object. It uses magnitude and angles of the gradient to compute the features, which will contribute to increasing accuracy of our model.\n\n### Step 1: \nPreprocessing. In this step, we would convert each img a 48 x 48 matrix to calculate gradient.","metadata":{}},{"cell_type":"code","source":"to_matrix = function(image_matrices, col = 48, row = 48) {\n  \n  image_pixels = list()\n  \n  for(i in 1:nrow(image_matrices)) {\n    \n    # extract pixel matrix for each image\n    image_pixels[[i]] = image_matrices[i, ] %>% matrix(ncol = col, nrow = row) \n  }\n  return(image_pixels)\n}\n\ndim(to_matrix(X_combined)[[1]])","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:01:45.18824Z","iopub.execute_input":"2022-10-19T22:01:45.189612Z","iopub.status.idle":"2022-10-19T22:01:45.594771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2:\nCalculate the Gradient Images. To calculate a HOG descriptor, we need to first calculate the horizontal and vertical gradient. Then, we can calculate magnitude and direction of gradient by following formula, in which `g` represents magnitude, and `Î¸` represents direction in degrees. Magnitude indecates the intensity of the direction change and direction indicates the angle of the change at a point.","metadata":{}},{"cell_type":"code","source":"# calculate horizontal and vertical gradients\nget_gradients <- function(image_matrices, size = 48) {\n    add_zeros_column = matrix(rep(0, size), ncol = 1)\n    add_zeros_row = matrix(rep(0, size), nrow = 1)\n    h_gradient = v_gradient = list()\n  \n    for(i in 1:length(image_matrices)) {\n    \n        removed_column <- image_matrices[[i]][,-size]\n        h_gradient[[i]] <- image_matrices[[i]] - cbind(add_zeros_column, removed_column)\n        \n        removed_row <- image_matrices[[i]][-1, ]\n        v_gradient[[i]] <- image_matrices[[i]] - rbind(removed_row, add_zeros_row)\n    \n    }\n    return(list(h = h_gradient, v = v_gradient))\n}","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:01:45.597101Z","iopub.execute_input":"2022-10-19T22:01:45.59843Z","iopub.status.idle":"2022-10-19T22:01:45.61148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 3:\nCalculate Histogram of Gradients in cells. Our facial data has all pictures in 48 Ã— 48 pixels, so we choose to calculate histograms for every 4 Ã— 4 pixels cell, because they can well capture the shape of faces. Thus, we had 12 Ã— 12 cells for each image. We used 9 bins to divide the whole range of direction (0: 180), and each pixels in a cell would be weighted by magnitue and allocated to two adjacent bins based on the direction position in the scale.","metadata":{}},{"cell_type":"code","source":"# calculate magnitude and direction of gradient\nget_magnitude <- function(h_gradient, v_gradient) {\n    magnitude <- list()\n  \n    for(i in 1:length(h_gradient)) {\n        magnitude[[i]] <- sqrt(h_gradient[[i]]^2 + v_gradient[[i]]^2)\n    }\n    return(magnitude)\n}","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:01:45.61378Z","iopub.execute_input":"2022-10-19T22:01:45.615123Z","iopub.status.idle":"2022-10-19T22:01:45.627255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 4:\nBlock Normalization. In the previous step, we calculate the histogram by \"counting\" and \"weighting\" the direction, so the histogram would be influenced by \"counting\" numbers a lot. Thus, we would normalize the histograms to specify the change on edges and corners. To do so, we chose to normalize the cells on larger 8 Ã— 8 pixels blocks, which contain 4 cells each, and there are 11 Ã— 11 blocks for each image. To make the vector as one-dimension, we also concatenated the each block to a 36 Ã— 1 vector, as in total, 36 Ã— 11 Ã— 11 = 4356.","metadata":{}},{"cell_type":"code","source":"get_direction <- function(h_gradient, v_gradient) {\n    direction <- list()\n    for(i in 1:length(h_gradient)) {\n        \n        direction[[i]] <- (atan(v_gradient[[i]] / h_gradient[[i]]))* (180/pi)\n        \n        # replace NAs with 0\n        direction[[i]][is.na(direction[[i]])] <- 0\n        \n        index <- direction[[i]] < 0\n        direction[[i]][index] <- direction[[i]][index] + 180\n    }\n    return(direction)\n}","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:01:45.629596Z","iopub.execute_input":"2022-10-19T22:01:45.630946Z","iopub.status.idle":"2022-10-19T22:01:45.642745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 5:\nCalculate the Histogram of Oriented Gradients feature vector. After calculating an image with all steps above, we made a function to loop over all images to calculate the whole feature vector.","metadata":{}},{"cell_type":"code","source":"# a function to calculate his of a img from direction and magnitude matrics:\nget_block_hist <- function(direction, magnitude){\n    \n    upper_limit <- seq(20, 160, 20)\n    lower_limit <- seq(0, 140, 20)\n    column <- list()\n    row_blocks <- list()\n  \n    # separate image into blocks\n    pixels <- seq(0, 44, by = 4)\n  \n    # loop over columns in 4*4 blocks\n    for(c in pixels){\n    \n    # loop over rows in 4*4 blocks\n    for(r in pixels){ \n        \n        # select a cell\n        direction_block <- direction[(r + 1):(r + 4), (c + 1):(c + 4)]\n        magnitude_block <- magnitude[(r + 1):(r + 4), (c + 1):(c + 4)]\n      \n        # because the intervals are closed to the right the last break point \n        # is set \n        # manually so that values of 180 are included\n        bin_index <- cut(direction_block, breaks = c(seq(0, 160, 20), 181), \n                        right = FALSE, labels = FALSE)\n        bin_index_matrix <- matrix(bin_index, 4, 4)\n        \n        # the bins are emptied for every matrix\n        bin <- numeric()\n      \n        for(j in 1:9){\n            \n            # special case bin 9\n            if(j == 9){\n          \n                # sum(magnitude_vector * ((180 - direction_vector) / 20))\n                bin[j] <- sum(c(magnitude_block[bin_index_matrix == j]) * \n                             ((180 - c(direction_block[bin_index_matrix == j])) / 20))\n          \n                # bin_content + sum(magnitude_vector * ((direction_vector - 0) / 20))\n                bin[1] <- bin[1] + sum(c(magnitude_block[bin_index_matrix == j]) * \n                                      ((c(direction_block[bin_index_matrix == j]) - 0)/20))# 0 or 160?\n                \n                # the last step for each bin, store the sum of the bin in colunm\n                row_blocks[[which(pixels == r)]] <- bin\n                column[[which(pixels == c)]] <- row_blocks\n          \n            }\n            \n            # sum(magnitude_vector * ((upper_limit - direction_vector)/20))\n            bin[j] <- sum(c(magnitude_block[bin_index_matrix == j]) * \n                         ((upper_limit[j] - c(direction_block[bin_index_matrix == j])) / 20))\n            \n            # sum(magnitude_vector * ((direction_vector - lower_limit)/20))\n            bin[j + 1] <- sum(c(magnitude_block[bin_index_matrix == j]) * \n                             ((c(direction_block[bin_index_matrix == j]) - lower_limit[j]) / 20))\n        }\n    }\n    }\n    return(column)\n}","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:01:45.645067Z","iopub.execute_input":"2022-10-19T22:01:45.646382Z","iopub.status.idle":"2022-10-19T22:01:45.659226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 8*8 Block Normalization\n# we have 12 blocks in a row so 11 big blocks for normalization when we use double pixels as row and col\nconcatenate_block_HOGs <- function(blocks) {\n  \n    concatenated_columns <- list()\n    concatenated_all <- list()\n  \n    # loop over column\n    for(i in 1:11) {\n    \n        # loop over row\n        for(j in 1:11){\n      \n            vect <- unlist(as.vector(c(blocks[[i]][j], blocks[[i]][j + 1],\n                                                           blocks[[i + 1]][j], blocks[[i + 1]][j + 1])))\n            # Normalized vector\n            concatenated_columns[[j]] <- vect/sum(sqrt(vect^2))\n        }\n        concatenated_all[[i]] <- concatenated_columns\n    }\n    # 9*4*11*11 = 4356\n    HoG_matrix <- matrix(unlist(concatenated_all), 1, 4356)\n    return(HoG_matrix)\n}","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:01:45.661543Z","iopub.execute_input":"2022-10-19T22:01:45.662909Z","iopub.status.idle":"2022-10-19T22:01:45.675436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the Histogram of Oriented Gradients feature vector\n# Calculate all img features\nget_all_hog <- function(img){\n    \n    HOGs_feature_matrix <- matrix(0, nrow(img), 4356)\n  \n    # get all image matrices in the right format\n    image_matrices <- to_matrix(img)\n  \n    # get h & v gradients\n    h_gradient <- get_gradients(image_matrices)$h\n    v_gradient <- get_gradients(image_matrices)$v\n  \n    img_dir <- get_direction(h_gradient, v_gradient)\n    img_mag <- get_magnitude(h_gradient, v_gradient)\n    \n    for(i in 1:nrow(img)) { \n    \n    HOGs <- get_block_hist(img_dir[[i]], img_mag[[i]])\n    \n    HOGs_feature_matrix[i,] <- concatenate_block_HOGs(HOGs)\n\n  }\n    colnames(HOGs_feature_matrix) <- paste(\"HOG\", 1:4356)\n    return(HOGs_feature_matrix)\n}","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:01:45.677797Z","iopub.execute_input":"2022-10-19T22:01:45.679194Z","iopub.status.idle":"2022-10-19T22:01:45.692004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply X_combined to the function to extract HOG_features:","metadata":{}},{"cell_type":"code","source":"# Extract\nHOG_features <- get_all_hog(X_combined)\nrownames(HOG_features) <- rownames(X_combined)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:01:45.694461Z","iopub.execute_input":"2022-10-19T22:01:45.695835Z","iopub.status.idle":"2022-10-19T22:03:20.555724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change to tibble format and add id as a column:","metadata":{}},{"cell_type":"code","source":"HOG_features2 <- as_tibble(HOG_features)\nHOG_features2 <- rownames_to_column(HOG_features2, \"id\")\nHOG_features2 %>%\n    head()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:03:20.558199Z","iopub.execute_input":"2022-10-19T22:03:20.559606Z","iopub.status.idle":"2022-10-19T22:03:21.167639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data preparation \n### Combining and Cleaning\n### 4.1. Data Frames Merging\n\nMerge spectral features for the whole image, freyslate features, histogram spectral features for edge features, and histogram of oriented gradients features into one data frame.","metadata":{}},{"cell_type":"code","source":"combined_df <- X_combined %>%\n    as_tibble(rownames = \"id\") %>%\n    left_join(spec_df, by = \"id\") %>%\n    left_join(freyslate_df, by = \"id\") %>%\n    left_join(histogram_df, by = \"id\") %>%\n    left_join(HOG_features2, by = \"id\") %>%\n    select(-id)\n\n# Check result\ncombined_df[1:6,2348:2358] %>% print\ndim(combined_df)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:03:21.171252Z","iopub.execute_input":"2022-10-19T22:03:21.172859Z","iopub.status.idle":"2022-10-19T22:03:22.519395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2. Data cleaning: \nUsing obervation and caret package to remove redundant variables\n\n* Variables that give NAs\n* Variables with near zero variation\n    - Have all same values or all NAs\n    - Compare most frequent to second most frequent value\n    - Compare number of unique values to number of observations\n* Variables that are highly correlated\n    - High correlation between variables provide similar or same information\n    - May be an indicator of instability of coefficient estimates","metadata":{}},{"cell_type":"code","source":"# replace NAs with 0\ncombined_df <- combined_df %>%\n    replace(is.na(.), 0)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:03:22.52176Z","iopub.execute_input":"2022-10-19T22:03:22.523128Z","iopub.status.idle":"2022-10-19T22:03:27.977957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before the further preprocessing, a `Principal Components Analysis (PCA)` is calculated over the whole set of features to get an idea of how many \"meaningful dimensions\" are among the predictors. ","metadata":{}},{"cell_type":"code","source":"zv_PCA <- c(which(apply(combined_df, 2, var)==0))\nzv_PCA\n## Id and these columns need to be removed before calculating PCA because they are constant or have\n## Zero Var, prevents prcomp to execute\ndf_PCA <- prcomp(combined_df[,-zv_PCA], scale = TRUE)\n## How many Principal components have an eigenvalue > 1?\npaste0(\"A PCA on the complete set of our predictors results in \", \n       length(which(df_PCA$sdev > 1)), \" principal components\")","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:03:27.980283Z","iopub.execute_input":"2022-10-19T22:03:27.981601Z","iopub.status.idle":"2022-10-19T22:03:40.628498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's get to preprocessing:","metadata":{}},{"cell_type":"code","source":"# Near zero variation\nnearzeroval <- combined_df %>%\n    caret::nearZeroVar(names = TRUE)\n\n# Remove nearzeroval variables\ncombined_df_cleaned <- combined_df %>%\n    select(-all_of(nearzeroval))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:03:40.632488Z","iopub.execute_input":"2022-10-19T22:03:40.634922Z","iopub.status.idle":"2022-10-19T22:04:06.10738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# High correlations\ncor_val <- combined_df_cleaned %>%\n    cor() %>%\n    replace(is.na(.), 0) # cor(0, 0) results in NAs so we replace them with 0\nvar <- caret::findCorrelation(cor_val, 0.99)\n\n# Remove nearzeroval and var variables\ncombined_df_cleaned <- combined_df_cleaned %>%\n    select(-all_of(var))\n\n# print the dimensions\ndimdf <- dim(combined_df_cleaned)\nprint(paste0(\"This is the current dimensions of the dataframe: \", dimdf[1], \" x \", dimdf[2], sep = ''))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:04:06.111244Z","iopub.execute_input":"2022-10-19T22:04:06.113458Z","iopub.status.idle":"2022-10-19T22:04:22.020273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What can we learn from this:\n* The number of remaining features after cleaning is very far away from the number of dimensions calculated with a principal component analysis. However, when being more strict with the correlation threshold, the models calculated in the following steps perform significantly worse when predicting test data. Thus, dimension reduction by means of a PCA is not advisable in this case","metadata":{}},{"cell_type":"markdown","source":"# 5. Splitting data for training & validation sets\n\nFirst, we divide training and test data into different dataframes\n* Two for full dataset\n* Two for clean dataset after removing `near zero variance` and `high correlations`","metadata":{}},{"cell_type":"code","source":"train_full_df <- combined_df[1:nrow(X_train), ]\ntest_full_df <- combined_df[-c(1:nrow(train_full_df)), ]\n\nclean_train_df <- combined_df_cleaned[1:nrow(X_train), ]\nclean_test_df <- combined_df_cleaned[-c(1:nrow(train_full_df)), ]","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:04:22.022644Z","iopub.execute_input":"2022-10-19T22:04:22.023972Z","iopub.status.idle":"2022-10-19T22:04:22.263193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check if the test and training datasets have same column lengths:","metadata":{}},{"cell_type":"code","source":"dim_train_full <- dim(train_full_df)\ndim_test_full <- dim(test_full_df)\nprint(paste0(\"This is the current column lengths of the full training dataset is: \", dim_train_full[2], \" and for full test dataset is \", dim_test_full[2], sep = ''))\n\ndim_clean_train <- dim(clean_train_df)\ndim_clean_test <- dim(clean_test_df)\nprint(paste0(\"This is the current column lengths of the clean training dataset is: \", dim_clean_train[2], \" and for clean test dataset is \", dim_clean_test[2], sep = ''))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:04:22.265802Z","iopub.execute_input":"2022-10-19T22:04:22.267328Z","iopub.status.idle":"2022-10-19T22:04:22.293174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we're going to divide the training df into 20% validation set and 80% training dataset, as it will yield more accurate results:","metadata":{}},{"cell_type":"code","source":"trainind <- caret::createDataPartition(y, p = 0.8, list = FALSE)\n\n# Train\n# Use below for model fitting!\ntrain_X <- clean_train_df[trainind, ]\ntrain_Y <- y[trainind]\n\n# Test\ntest_X <- clean_train_df[-trainind, ]\ntest_Y <- y[-trainind]","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:04:22.296854Z","iopub.execute_input":"2022-10-19T22:04:22.298452Z","iopub.status.idle":"2022-10-19T22:04:22.408254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Model fitting\n\nTo figure out which model provides the best trade off between bias and variance, between accuracy and flexibility, one strategy is to fit both a flexible and a more rigid model and determine from CV error which direction on the flexiblity axis we should go to avoid overtraining.","metadata":{}},{"cell_type":"code","source":"suppressMessages(require(caret))\n\n## Use multiple cores whenever possible\nlibrary(parallel)\nsuppressMessages(library(doParallel))\n(detectCores() - 1) %>% makeCluster() %>% registerDoParallel()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:04:22.410568Z","iopub.execute_input":"2022-10-19T22:04:22.411879Z","iopub.status.idle":"2022-10-19T22:04:24.461069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set traincontrol for cross-validation\ntrCntrl = trainControl('cv', 5, allowParallel = TRUE)","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:04:24.463472Z","iopub.execute_input":"2022-10-19T22:04:24.464949Z","iopub.status.idle":"2022-10-19T22:04:24.477543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.1. Linear Discriminant Analysis (LDA)\n\nAs our first model we tried Linear Discriminant Analysis (LDA).\nLDA is a linear model used for classification and dimensionality reduction. First created for two classes in 1936 by R. Fisher and then generalized for multiple classes by C.R. Rao. It is used to solve various classification problems. \n\nA good example is given by the book: think of an online banking service trying to determine whether or not a transaction being performed is fraudelent or not, on the basis of the user's IP address, past transaction history, and so forth (James et al., 2013).  \n\nIn our own problem we also try to classify, but this time 4 classes and have many different features. For the LDA we assume that X is drawn from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix (James et al., 2013).\n\nUnfortunately in our notebook LDA failed to run succesfully.","metadata":{}},{"cell_type":"code","source":"# LDA does not function ! \n#set.seed(2020)\n#fit_lda_f = train(train_X, train_Y, method='lda', trControl=trCntrl)\n#fit_lda_f\n","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:04:24.479713Z","iopub.execute_input":"2022-10-19T22:04:24.480986Z","iopub.status.idle":"2022-10-19T22:04:24.491055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2. Quadratic Discriminant Analysis (QDA)\nThe main difference between LDA and Quadratic Discriminant Analysis (QDA) is that QDA assumes that each class has its own covariance matrix, whereas LDA assumes that each class has the same covariance matrix.\n\nQDA is a much more flexible classifier than LDA and has substantially higher variance. This could potentially lead to better prediction performance compared to LDA (James et al., 2013).\n\nIn general, QDA is recommended if the training set is very large. In our case, our training set is definitely large enough.\n\nWhen trying to use QDA that was not pre-processed, it failed (without PCA or Ridge regularization).","metadata":{}},{"cell_type":"code","source":"# QDA with PCA as preprocessing step\nset.seed(2020)\nfit_qda_f_pca = train(train_X, train_Y, method='qda', trControl=trCntrl, preProcess = \"pca\")\nfit_qda_f_pca","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:04:24.493242Z","iopub.execute_input":"2022-10-19T22:04:24.494553Z","iopub.status.idle":"2022-10-19T22:05:03.848225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Since the accuracy is not as high for QDA (.82) as for the best performing models, ridge regularized QDA was not tested*","metadata":{}},{"cell_type":"markdown","source":"## 6.3. K-nearest neighbors (KNN)\nK-nearest neighbors (KNN) is a type of supervised learning algorithm used for both regression and classification. KNN tries to predict the correct class for the test data by calculating the distance between the test data and all the training points. Then select the K number of points which is closet to the test data. The KNN algorithm calculates the probability of the test data belonging to the classes of â€˜Kâ€™ training data and class holds the highest probability will be selected. In the case of regression, the value is the mean of the â€˜Kâ€™ selected training points. Source: https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4","metadata":{}},{"cell_type":"code","source":"## KNN (unscaled)\nset.seed(2020)\nfit_knn = train(train_X, train_Y, method='knn', trControl=trCntrl)\nplot(fit_knn)\nfit_knn","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:05:03.852092Z","iopub.execute_input":"2022-10-19T22:05:03.854464Z","iopub.status.idle":"2022-10-19T22:08:00.320308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Highest accuracy within this model was reached with k = 7, however rather poor (.519)*","metadata":{}},{"cell_type":"code","source":"## KNN (scaled)\nset.seed(2020)\nfit_knn_s = train(train_X, train_Y, method='knn', trControl=trCntrl, preProcess = \"scale\")\nplot(fit_knn_s)\nfit_knn_s","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:08:00.322849Z","iopub.execute_input":"2022-10-19T22:08:00.32427Z","iopub.status.idle":"2022-10-19T22:11:12.899966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Highest accuracy within this model was reached with k = 9, a good bit higher (.698) than\nin unscaled knn*","metadata":{}},{"cell_type":"markdown","source":"## 6.4. The Lasso\nNext up is the Lasso. The Lasso is a shrinkage method and a clear improvement over the Ridge regression. Ridge regression will always include all $p$ predictors in the final model. This is because the penalty in ridge regression will shrink all of the coefficients very very close to zero, but it will not actually set any of them to actual zero. This is an issue in model interpretation when the number of variables $p$ is quite large (James et al., 2013).\n\nThe Lasso uses a different penalty and shrinks all coefficient estimates to zero, however the $l1$ penalty forces some of the estimates down to exactly zero as the tuning parameter Lambda increases. This results in variable selection (James et al., 2013).","metadata":{}},{"cell_type":"code","source":"# Fit a cross-validated Lasso\nset.seed(2020)\nfit_lasso <- glmnet::cv.glmnet(as.matrix(train_X), as.matrix(train_Y),\n                      family = \"multinomial\",\n                      type.measure = \"class\",\n                      nfolds = 5,\n                      parallel = TRUE,\n                      alpha = 1)\nfit_lasso","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:11:12.90237Z","iopub.execute_input":"2022-10-19T22:11:12.903765Z","iopub.status.idle":"2022-10-19T22:11:39.101896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictions with the tuned LASSO-model on the training set\npred_lasso <- predict(fit_lasso, as.matrix(test_X), \n                          s = \"lambda.min\", \n                          type = 'class', \n                          alpha = 1) %>% \n                          as.factor()\n\ncaret::confusionMatrix(pred_lasso, factor(test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:11:39.104199Z","iopub.execute_input":"2022-10-19T22:11:39.105535Z","iopub.status.idle":"2022-10-19T22:11:39.22738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the optimal lambda (0.01898), LASSO regression reached an accuracy of .78 on our pre defined test set","metadata":{}},{"cell_type":"markdown","source":"## 6.5. Ridge Regression\nRidge regression is an improvement over least squares as it uses a tuning parameter called Lambda. As Lambda increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias (James et al., 2013).\n\nEven though the Lasso is a general improvement to ridge regression, we want to include ridge regression to make sure we try out all possible models. \n","metadata":{}},{"cell_type":"code","source":"fit_ridge <- train(train_X, train_Y, \n                   method = 'glmnet', \n                   trControl = trCntrl, \n                   tuneGrid = expand.grid(alpha = 0, lambda = 0.002))\nfit_ridge","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:11:39.229659Z","iopub.execute_input":"2022-10-19T22:11:39.230963Z","iopub.status.idle":"2022-10-19T22:12:22.716256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict the fitted model:","metadata":{}},{"cell_type":"code","source":"pred_ridge <- predict(fit_ridge, test_X, type ='raw') \nconfusionMatrix(pred_ridge, factor(test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:12:22.718578Z","iopub.execute_input":"2022-10-19T22:12:22.719898Z","iopub.status.idle":"2022-10-19T22:12:22.844829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ridge Regression with a constant lambda of 0.002 reached an accuracy of .83 on our pre defined test set","metadata":{}},{"cell_type":"markdown","source":"## 6.6. Classification tree\nA classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one. Re-call that for a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the\nsame terminal node. \n\nIn contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. In interpreting the results of a classification tree, we are often interested not only in the class prediction corresponding to a particular terminal node region, but also in the class proportions among the training observations that fall into that region (James et al., 2013).","metadata":{}},{"cell_type":"code","source":"## Fit a CART using 5-fold cross-validation to tune the complexity parameter\n## (fixed cP parameter was taken over from teachers example implemented in \n## the raw version of this notebook)\nset.seed(2020) \ntt <- Sys.time()\nfittree = train(x=train_X, y=train_Y, method='rpart', trControl = trCntrl,\n                tuneGrid = data.frame(cp=.02))\nfittree\n(dur <- Sys.time() - tt)\nfittree$results$Accuracy","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:12:22.847101Z","iopub.execute_input":"2022-10-19T22:12:22.848453Z","iopub.status.idle":"2022-10-19T22:12:33.471646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Tuning the cP parameter\nsuppressWarnings({\nset.seed(2020)\ncps <- seq(0.001, 0.01, 0.001)\ncp_acc <- as.matrix(cps, byrow = TRUE)\nacc_s <- c()   \nfor (i in cps) {\n    acc <- train(x=train_X, y=train_Y, method='rpart', trControl = trCntrl,\n                    tuneGrid = data.frame(cp = i))$results$Accuracy\n    acc_s <- c(acc_s, acc)\n    \n   \n}\nacc_s <- as.matrix(acc_s, byrow = TRUE)                    \ntune <- cbind(cp_acc, acc_s)\nprint(tune)\n})","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:12:33.473883Z","iopub.execute_input":"2022-10-19T22:12:33.475223Z","iopub.status.idle":"2022-10-19T22:14:28.690753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Iterative usage of different cPs gave cP = 0.003 as best value with the best outcome in terms of \naccuracy","metadata":{}},{"cell_type":"code","source":"## Fitting the CART again with optimized cP\nset.seed(2020) \nfittree_tune = train(x=train_X, y=train_Y, method='rpart', trControl = trCntrl,\n                     tuneGrid = data.frame(cp=0.003))\nfittree_tune","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:14:28.693122Z","iopub.execute_input":"2022-10-19T22:14:28.69449Z","iopub.status.idle":"2022-10-19T22:14:40.653131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*The optimized CART yielded an accuracy of .62*","metadata":{}},{"cell_type":"code","source":"## Check performance on our defined test set\npredtree = predict(fittree_tune, test_X, type='raw') \nconfusionMatrix(predtree, factor(test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:14:40.655696Z","iopub.execute_input":"2022-10-19T22:14:40.657175Z","iopub.status.idle":"2022-10-19T22:14:40.972172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Predictions accuracy on our predefined test set was a bit below (.59) the predictions on the training set*","metadata":{}},{"cell_type":"markdown","source":"## 6.7. Support Vector Machine:\nThe objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N â€” the number of features) that distinctly classifies the data points.\n\nTo separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n\nHyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.\n\nSupport vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM. \n\nSource: https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47","metadata":{}},{"cell_type":"code","source":"set.seed(2020)\ntune_svm = e1071::tune.svm(y ~., \n                       data = data.frame(train_X, y = as.factor(train_Y)), \n                       kernel = 'radial',\n                       gama = c(0.5,1,2,3,4),\n                       range = list(cost = c(0.001,0.01,0.1,1,5,10, 100)))\n\ntune_svm$best.model","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:14:40.975659Z","iopub.execute_input":"2022-10-19T22:14:40.977188Z","iopub.status.idle":"2022-10-19T22:22:42.110089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We choose cost = 1 because it allows us to use the most accurate/ best model**","metadata":{}},{"cell_type":"code","source":"set.seed(2020)\nfit_svm = e1071::svm(y~., data = data.frame(train_X, y = factor(train_Y)), cost = 1, \n                scale = TRUE, kernel = 'radial')\n\npred_svm = predict(fit_svm, test_X, type = 'raw')\ncaret::confusionMatrix(pred_svm, factor(test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:22:42.112368Z","iopub.execute_input":"2022-10-19T22:22:42.113715Z","iopub.status.idle":"2022-10-19T22:23:35.099905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predictive accuracy of the optimal SVM does quite well, .905 accuracy on our predefined test data, **the best accuracy score so far**.","metadata":{}},{"cell_type":"markdown","source":"## 6.8. Random Forests\nBagging involves creating multiple copies of the original train-\ning data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model. Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. The split is allowed to use only one of those $m$ predictors. A fresh sample of\n$m$ predictors is taken at each split (James et al., 2013).\n\nRandom forests are probably the least susceptible to overtraining and is considered one of the best \"off the shelf\" machine learning algorithms in the sense that they require little expertise in application, and easily perform well without tuning. ","metadata":{}},{"cell_type":"code","source":"## Fitting a random forest (Without auto tuning the meta parameters)\nset.seed(2020)\nfit_rf = caret::train(train_X, train_Y, method='ranger', \n    trControl = trCntrl, \n    tuneGrid = data.frame(mtry=9, splitrule=\"gini\", min.node.size=10)\n)\nfit_rf$finalModel","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:23:35.102115Z","iopub.execute_input":"2022-10-19T22:23:35.10343Z","iopub.status.idle":"2022-10-19T22:23:47.049475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predrf = predict(fit_rf, test_X) \nconfusionMatrix(predrf, factor(test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:23:47.051663Z","iopub.execute_input":"2022-10-19T22:23:47.052983Z","iopub.status.idle":"2022-10-19T22:23:47.322356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predictive accuracy of the random forest was .875 with no tuning, however further tuning might yield even better performance","metadata":{}},{"cell_type":"code","source":"## Tuning of the grid parameters (number of trees kept constant)\n## (Different intermediate steps of altering mtry and min.node.size were omitted here, \n## mtry was tested from values 1 to 12, min node size was tested from\n## values 1 to 20)\n\ntgrid <- expand.grid(\n  .mtry = 12,\n  .splitrule = \"gini\",\n  .min.node.size = c(1, 2, 5))\n\nfit_rf_tune1 = caret::train(train_X, train_Y, method='ranger', \n    trControl = trCntrl, \n    tuneGrid = tgrid)\n\nfit_rf_tune1$finalModel","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:23:47.325754Z","iopub.execute_input":"2022-10-19T22:23:47.327338Z","iopub.status.idle":"2022-10-19T22:24:26.872689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keeping the number of trees constant, the optimal random forest seems to\narise from mtry = 12 and minimal node size 1","metadata":{}},{"cell_type":"code","source":"predrf_tune1 = predict(fit_rf_tune1, test_X) \nconfusionMatrix(predrf_tune1, factor(test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:24:26.874989Z","iopub.execute_input":"2022-10-19T22:24:26.876324Z","iopub.status.idle":"2022-10-19T22:24:27.152974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With constant number of trees and optimal mtry and min node size, \nan accuracy of .88 is reached\n","metadata":{}},{"cell_type":"code","source":"## Fitting optimized random forest:\n## Tuning the number of trees\nsuppressWarnings({\nset.seed(2020)\nnum_trees <- seq(500, 2000, 100)\ntrees_acc <- as.matrix(num_trees, byrow = TRUE)\nacc_t <- c()   \nfor (i in num_trees) {\n    oob_tree <- train(train_X, train_Y, method='ranger', \n    trControl = trCntrl, \n    tuneGrid = data.frame(mtry=12, splitrule=\"gini\", min.node.size=1),\n    num.trees = i)$finalModel$prediction.error\n    acc_t <- c(acc_t, oob_tree)\n}\nacc_t <- as.matrix(acc_t, byrow = TRUE)                    \ntuned_trees <- cbind(trees_acc, acc_t)\nprint(tuned_trees)\n})","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:24:27.15654Z","iopub.execute_input":"2022-10-19T22:24:27.158244Z","iopub.status.idle":"2022-10-19T22:33:30.19764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lowest OOB error in the range of inspected number of trees arises from 1900 trees, however result is likely to vary with new run since the values are very close to each other","metadata":{}},{"cell_type":"code","source":"set.seed(2020)\nfit_rf_king = train(train_X, train_Y, method='ranger', \n    trControl = trCntrl, \n    tuneGrid = data.frame(mtry=12, splitrule=\"gini\", min.node.size=1), \n    num.trees = 1900)\n\nfit_rf_king$finalModel","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:33:30.19994Z","iopub.execute_input":"2022-10-19T22:33:30.201273Z","iopub.status.idle":"2022-10-19T22:34:23.271153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predrf_tune_king = predict(fit_rf_king, test_X) \nconfusionMatrix(predrf_tune_king, factor(test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:34:23.273482Z","iopub.execute_input":"2022-10-19T22:34:23.274868Z","iopub.status.idle":"2022-10-19T22:34:23.738274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predictive accuracy on the test set did not increase compared with the 500 trees, still around .88","metadata":{}},{"cell_type":"markdown","source":"## 6.9. eXtreme Gradient Boosted Tree\nBoosting is quite similar to bagging. In bagging, each tree is built on a bootstrap data set, independent of the other trees. \n\nBoosting works in a similar way, except that the trees are\ngrown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\n\nUnlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead *learns slowly* (James et al., 2013).","metadata":{}},{"cell_type":"code","source":"## fitting a boosted tree (without changing the meta parameters)\nset.seed(2020)\nfit_xgb = train(train_X, train_Y, method=\"xgbTree\", \n    trControl = trCntrl, \n    tuneGrid = data.frame(\n        nrounds=600, max_depth=4, eta=.3, \n        gamma=0, colsample_bytree=0.95, \n        min_child_weight=1, subsample=1)\n)\nfit_xgb","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:34:23.74054Z","iopub.execute_input":"2022-10-19T22:34:23.7419Z","iopub.status.idle":"2022-10-19T22:58:42.983813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Validation set estimate for xgb\npred_xgb = predict(fit_xgb, test_X) \nconfusionMatrix(pred_xgb, factor(test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:58:42.986002Z","iopub.execute_input":"2022-10-19T22:58:42.987329Z","iopub.status.idle":"2022-10-19T22:58:43.096467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After very long calculation time, Extreme gradient boosting yields an accuracy of \n0.91 with all the indicated tuning parameters. In the submission on the test data, Extreme boosting performed slightly worse than Random Forest and SVM. Therefore, we decided against further tuning of this model","metadata":{}},{"cell_type":"markdown","source":"# 7. Model comparison & conclusion","metadata":{}},{"cell_type":"code","source":"# accuracy scores for each model saved in vector\n\nlasso_confusion <- confusionMatrix(pred_lasso, factor(test_Y))\nridge_confusion <- confusionMatrix(pred_ridge, factor(test_Y))\ntree_confusion <- confusionMatrix(predtree, factor(test_Y))\nsvm_confusion <- confusionMatrix(pred_svm, factor(test_Y))\nrf_confusion <- confusionMatrix(predrf, factor(test_Y))\nrf_tune_confusion <- confusionMatrix(predrf_tune_king, factor(test_Y))\nxgb_confusion <- confusionMatrix(pred_xgb, factor(test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:58:43.098654Z","iopub.execute_input":"2022-10-19T22:58:43.099976Z","iopub.status.idle":"2022-10-19T22:58:43.191646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# credit to group 1\n\nmodels <- rbind(lasso_confusion$overall, ridge_confusion$overall, tree_confusion$overall,\n                svm_confusion$overall, \n                rf_confusion$overall, rf_tune_confusion$overall, xgb_confusion$overall) %>%\n        as_tibble %>%\n        add_column(Model = c(\"Lasso\", \"Ridge\", \"Classification Tree\", \"SVM\", \"Random Forest\",\n                             \"T-Random Forest\", \"Boosted Tree\"))\n\nmodels %>%\n    ggplot(aes(x = Model, y = Accuracy, fill = Model)) +\n    geom_col() +\n    # for getting the error bar\n    geom_errorbar(aes(ymax = AccuracyLower, ymin = AccuracyUpper)) +\n    theme_minimal() +\n    theme(axis.text = element_text(size = 10), axis.title = element_text(size = 13),\n          plot.title = element_text(size = 15, hjust = 0.5), legend.position = \"none\") +\n    labs(title = \"Accuracy Estimates for Each Model\")","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:58:43.194053Z","iopub.execute_input":"2022-10-19T22:58:43.195446Z","iopub.status.idle":"2022-10-19T22:58:43.555707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### References:\n\nP. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar and I. Matthews, \"The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression,\" *2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops*, 2010, pp. 94-101. DOI: 10.1109/CVPRW.2010.5543262. \n\nJack, R. E., Garrod, O. G. B., Yu, H., Caldara, R., & Schyns, P. G. (2012). *Facial expressions of emotion are not culturally universal. Proceedings of the National Academy of Sciences*, *109*(19), 7241â€“7244. DOI: 10.1073/pnas.1200155109\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Springer Publishing.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion:\n\nFrom the accuracy plots, we can observe that the boosted tree yields the highest accuracy. However, considering that boosted tree is more prone to overfitting than random forests, we chose random forests as our final model to fit. The sensitivity and specificity of random forests also look quite proper, showing it is appropriate to choose this as our final model.","metadata":{}},{"cell_type":"markdown","source":"# 8. Formatting your submission file\n\nTo format your submission file, you can use the following code:","metadata":{}},{"cell_type":"code","source":"## Make predictions\npred_svm = predict(fit_svm, clean_test_df, type='raw')\n\n## Write to file\ntibble(file = rownames(X_test), category = pred_svm) %>% \n    write_csv(path = \"submission.csv\")\n\n## Check result\ncat(readLines(\"submission.csv\",n=20), sep=\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-10-19T22:58:43.557998Z","iopub.execute_input":"2022-10-19T22:58:43.559419Z","iopub.status.idle":"2022-10-19T22:58:52.381931Z"},"trusted":true},"execution_count":null,"outputs":[]}]}